{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 184
    },
    "id": "U8nkwhWdNSCA",
    "outputId": "3068341c-a440-4d48-a70b-081b29687f9c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "import pickle\n",
    "import itertools as it\n",
    "from tensorflow.keras import backend as K, initializers, regularizers, constraints\n",
    "from tensorflow.keras.layers import Layer\n",
    "import tensorflow_addons as tfa\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from functools import partial\n",
    "import itertools\n",
    "import random\n",
    "import os\n",
    "import absl.logging\n",
    "import json\n",
    "from tqdm.auto import tqdm\n",
    "import logging\n",
    "\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "absl.logging.set_verbosity(absl.logging.ERROR)\n",
    "print(tf.__version__)\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "os.environ['PYTHONHASHSEED']=str(123)\n",
    "os.environ['TF_CUDNN_DETERMINISTIC'] = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "VDYJlQiaOHvR"
   },
   "outputs": [],
   "source": [
    "def keras_rmse(pred_index, y_true, y_pred):\n",
    "    \"\"\"if pred_index is not None:\n",
    "        y_pred = y_pred[pred_index]\"\"\"\n",
    "    return tf.keras.backend.sqrt(tf.keras.backend.mean(tf.keras.backend.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "\n",
    "EPSILON = 1e-10\n",
    "\n",
    "\n",
    "def _error(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\" Simple error \"\"\"\n",
    "    return actual - predicted\n",
    "\n",
    "\n",
    "def _percentage_error(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\"\n",
    "    Percentage error\n",
    "\n",
    "    Note: result is NOT multiplied by 100\n",
    "    \"\"\"\n",
    "    return _error(actual, predicted) / (actual + EPSILON)\n",
    "\n",
    "\n",
    "def _relative_error(actual: np.ndarray, predicted: np.ndarray, benchmark: np.ndarray = None):\n",
    "    \"\"\" Relative Error \"\"\"\n",
    "    if benchmark is None or isinstance(benchmark, int):\n",
    "        # If no benchmark prediction provided - use naive forecasting\n",
    "        if not isinstance(benchmark, int):\n",
    "            seasonality = 1e-10\n",
    "        else:\n",
    "            seasonality = benchmark\n",
    "        return _error(actual[seasonality:], predicted[seasonality:]) /\\\n",
    "               (_error(actual[seasonality:], _naive_forecasting(actual, seasonality)) + EPSILON)\n",
    "\n",
    "    return _error(actual, predicted) / (_error(actual, benchmark) + EPSILON)\n",
    "\n",
    "\n",
    "def mse(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\" Mean Squared Error \"\"\"\n",
    "    return np.mean(np.square(_error(actual, predicted)))\n",
    "\n",
    "\n",
    "def rmse(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\" Root Mean Squared Error \"\"\"\n",
    "    return np.sqrt(mse(actual, predicted))\n",
    "\n",
    "\n",
    "    return np.mean(_error(actual, predicted))\n",
    "\n",
    "\n",
    "def mae(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\" Mean Absolute Error \"\"\"\n",
    "    return np.mean(np.abs(_error(actual, predicted)))\n",
    "\n",
    "\n",
    "def mape(actual: np.ndarray, predicted: np.ndarray):\n",
    "    \"\"\"\n",
    "    Mean Absolute Percentage Error\n",
    "\n",
    "    Properties:\n",
    "        + Easy to interpret\n",
    "        + Scale independent\n",
    "        - Biased, not symmetric\n",
    "        - Undefined when actual[t] == 0\n",
    "\n",
    "    Note: result is NOT multiplied by 100\n",
    "    \"\"\"\n",
    "    return np.mean(np.abs(_percentage_error(actual, predicted)))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "METRICS = {\n",
    "    'mse': mse,\n",
    "    'rmse': rmse,\n",
    "    'mae': mae,\n",
    "    'mape': mape,\n",
    "}\n",
    "\n",
    "\n",
    "def evaluate(actual: np.ndarray, predicted: np.ndarray, metrics=('mae', 'rmse')):\n",
    "    results = {}\n",
    "    for name in metrics:\n",
    "        try:\n",
    "            results[name] = METRICS[name](actual, predicted)\n",
    "        except Exception as err:\n",
    "            results[name] = np.nan\n",
    "            print('Unable to compute metric {0}: {1}'.format(name, err))\n",
    "    return results\n",
    "\n",
    "\n",
    "tf.keras.utils.get_custom_objects().update({\"rmse\": rmse})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_type, window, horizon, d_model, n_features, batch_size, lamb1, lamb2, grad_smooth_alpha, mode, overall):\n",
    "    if model_type == 'encdecluong':\n",
    "        inp = tf.keras.layers.Input(shape=(window, n_features))\n",
    "        embedding = tf.keras.layers.Conv1D(d_model, n_features, activation='relu', input_shape=(window, 3), padding='causal')(inp)\n",
    "\n",
    "        out = tf.keras.layers.LSTM(d_model, activation='relu', return_sequences=True, return_state=True)(embedding)\n",
    "        out = tf.keras.layers.Attention()(out)\n",
    "        out = tf.keras.layers.LSTM(d_model, activation='relu')(out)\n",
    "        out = tf.keras.layers.Dense(horizon)(out)\n",
    "\n",
    "        model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "        loss = 'mse'\n",
    "\n",
    "    elif model_type == 'lstm':\n",
    "        inp = tf.keras.layers.Input(shape=(window, n_features))\n",
    "        embedding = tf.keras.layers.Conv1D(d_model//2, n_features, activation='relu', input_shape=(window, 3), padding='causal')(inp)\n",
    "        embedding = tf.keras.layers.Conv1D(d_model, n_features, activation='relu', padding='causal')(embedding)\n",
    "        out = tf.keras.layers.LSTM(d_model, activation='relu', return_sequences=True)(embedding)\n",
    "        out = tf.keras.layers.LSTM(d_model, activation='relu')(out)\n",
    "        out = tf.keras.layers.Dense(horizon)(out)\n",
    "\n",
    "        model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "        loss = 'mse'\n",
    "\n",
    "    elif model_type == 'lstm_1layer':\n",
    "        inp = tf.keras.layers.Input(shape=(window, n_features))\n",
    "        embedding = tf.keras.layers.Conv1D(d_model//2, n_features, activation='relu', input_shape=(window, 3), padding='causal')(inp)\n",
    "        embedding = tf.keras.layers.Conv1D(d_model, n_features, activation='relu', padding='causal')(embedding)\n",
    "        out = tf.keras.layers.LSTM(d_model, activation='relu')(embedding)\n",
    "        out = tf.keras.layers.Dense(horizon)(out)\n",
    "\n",
    "        model = tf.keras.Model(inputs=inp, outputs=out)\n",
    "\n",
    "        loss = 'mse'\n",
    "\n",
    "    elif model_type == 'hlnet':\n",
    "        pred_index = -1\n",
    "        inputs = tf.keras.layers.Input(shape=(window, 3))\n",
    "        group_factors = [6, 1]\n",
    "        group_steps = [1, 1]\n",
    "        dense_layers = []\n",
    "        outputs = []\n",
    "        recurrent_units = [d_model]\n",
    "        recurrent_dropout = 0\n",
    "        return_sequences = False\n",
    "        prev_hidden = None\n",
    "\n",
    "        for gf, ge in zip(group_factors, group_steps):\n",
    "            return_sequences_tmp = return_sequences if len(recurrent_units) == 1 else True\n",
    "            inputs_gr = tf.math.reduce_mean(tf.signal.frame(inputs, gf, ge, axis=1), axis=2)\n",
    "            x = tf.keras.layers.Conv1D(recurrent_units[0], 3, activation='relu', input_shape=(window, 3), padding='causal')(inputs_gr)\n",
    "\n",
    "            x = tf.keras.layers.LSTM(\n",
    "                recurrent_units[0],\n",
    "                return_sequences=return_sequences_tmp,\n",
    "            )(x)\n",
    "\n",
    "            for i, u in enumerate(recurrent_units[1:]):\n",
    "                return_sequences_tmp = (\n",
    "                    return_sequences if i == len(recurrent_units) - 2 else True\n",
    "                )\n",
    "                x = tf.keras.layers.LSTM(\n",
    "                    u, return_sequences=return_sequences_tmp, dropout=recurrent_dropout\n",
    "                )(x)\n",
    "\n",
    "            # Dense layers\n",
    "            if return_sequences:\n",
    "                x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "            if prev_hidden is not None:\n",
    "                x = tf.keras.layers.Concatenate(axis=1)([x, prev_hidden])\n",
    "            else:\n",
    "                x_hidden = tf.identity(x)\n",
    "                prev_hidden = tf.stop_gradient(x_hidden)\n",
    "\n",
    "            for hidden_units in dense_layers:\n",
    "                x = tf.keras.layers.Dense(hidden_units)(x)\n",
    "                if dense_dropout > 0:\n",
    "                    x = tf.keras.layers.Dropout(dense_dropout)(dense_dropout)\n",
    "\n",
    "            layer_out = tf.keras.layers.Dense(horizon - gf + 1, name=f'level_{gf}_out')(x)\n",
    "            outputs.append(layer_out)\n",
    "\n",
    "        model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        loss = {\n",
    "            f\"level_{gf}_out\": hierarchical_loss(base_criterion='mse', gf=gf, ge=ge, reduction=tf.keras.losses.Reduction.SUM)\n",
    "            for i, (gf, ge) in enumerate(zip(group_factors, group_steps))\n",
    "        }\n",
    "\n",
    "    \n",
    "    elif model_type == 'multitaskhlnet_taskslstm_firstlevelcnn':\n",
    "    #pred_index = -1\n",
    "\n",
    "        class MultitaskHLNet(tf.keras.Model):\n",
    "\n",
    "            def __init__(self, d_model, n_features, window, horizon, batch_size):\n",
    "                super(MultitaskHLNet, self).__init__()\n",
    "                self.d_model = d_model\n",
    "                self.n_features = n_features\n",
    "                self.window = window\n",
    "                self.horizon = horizon\n",
    "                self.batch_size = batch_size\n",
    "\n",
    "                self.global_embedding_1 = tf.keras.layers.Conv1D(d_model//2, 3, activation='relu', input_shape=(window, 3), padding='causal')\n",
    "                self.global_embedding_2 = tf.keras.layers.Conv1D(d_model, 3, activation='relu', padding='causal')\n",
    "                self.first_level_lstm_layer = tf.keras.layers.LSTM(d_model, return_sequences=True)\n",
    "\n",
    "                \"\"\"\n",
    "                    First Level tasks - Sequence featurization\n",
    "                \"\"\"\n",
    "                self.gap_embedding_1 = tf.keras.layers.Conv1DTranspose(d_model, 3, activation='relu', padding='same')\n",
    "                self.gap_embedding_2 = tf.keras.layers.Conv1DTranspose(d_model//2, 3, activation='relu', padding='same')\n",
    "                self.gap_filling_output = tf.keras.layers.Dense(n_features)\n",
    "                #self.gap_filling_lstm_layer = tf.keras.layers.LSTM(d_model, name='gap_filling_lstm', activation='relu')\n",
    "                #self.gap_filling_reshape = tf.keras.layers.Reshape((-1, self.window, self.n_features), name='gap_filling_task')\n",
    "                \n",
    "                self.noise_embedding_1 = tf.keras.layers.Conv1DTranspose(d_model, 3, activation='relu', padding='same')\n",
    "                self.noise_embedding_2 = tf.keras.layers.Conv1DTranspose(d_model//2, 3, activation='relu', padding='same')\n",
    "                self.gaussian_noise = tf.keras.layers.GaussianNoise(0.01)\n",
    "                #self.noise_reduction_lstm_layer = tf.keras.layers.LSTM(d_model, name='noise_reduction_lstm', activation='relu')\n",
    "                self.noise_reduction_output = tf.keras.layers.Dense(n_features)\n",
    "                #self.noise_reduction_reshape = tf.keras.layers.Reshape((-1, window, n_features), name='noise_reduction_task')\n",
    "\n",
    "                self.swap_combinations = list(itertools.permutations(np.arange(0, 4), 4))\n",
    "                self.swap_lstm_layer = tf.keras.layers.LSTM(d_model, name='swap_lstm', activation='relu')\n",
    "                self.swap_output = tf.keras.layers.Dense(len(self.swap_combinations), name='swap_task', activation='softmax')\n",
    "\n",
    "                #self.quarter_sequence = tf.keras.layers.Dense(4, activation='softmax', name='quarter_sequence')\n",
    "                #self.month_sequence = tf.keras.layers.Dense(12, activation='softmax', name='month_sequence')\n",
    "                #self.day_of_week_sequence = tf.keras.layers.Dense(7, activation='softmax', name='day_of_week_sequence')\n",
    "\n",
    "\n",
    "                \"\"\"\n",
    "                    Second Level tasks - Forecasting helpers\n",
    "                \"\"\"\n",
    "                self.second_level_lstm_layer = tf.keras.layers.LSTM(d_model, return_sequences=True)\n",
    "\n",
    "                self.smooth_lstm_layers = {'8': tf.keras.layers.LSTM(d_model, name='smooth_forecasting_8_lstm', activation='relu'),\n",
    "                                '6':  tf.keras.layers.LSTM(d_model, name='smooth_forecasting_6_lstm', activation='relu'),\n",
    "                                '3':  tf.keras.layers.LSTM(d_model, name='smooth_forecasting_3_lstm', activation='relu')}\n",
    "\n",
    "                self.smooth_layers = {'8': tf.keras.layers.Dense(horizon-8+1, name='smooth_forecasting_8'),\n",
    "                                '6': tf.keras.layers.Dense(horizon-6+1, name='smooth_forecasting_6'),\n",
    "                                '3': tf.keras.layers.Dense(horizon-3+1, name='smooth_forecasting_3')}\n",
    "\n",
    "\n",
    "                self.one_step_forecast_lstm_layers = {'0': tf.keras.layers.LSTM(d_model, name='next_step_forecasting_lstm', activation='relu'),\n",
    "                                str(horizon//2):  tf.keras.layers.LSTM(d_model, name='mid_step_forecasting_lstm', activation='relu'),\n",
    "                                str(horizon-1):  tf.keras.layers.LSTM(d_model, name='last_step_forecasting_lstm', activation='relu')}\n",
    "\n",
    "                self.one_step_forecast = {'0': tf.keras.layers.Dense(1, name='next_step_forecasting'),\n",
    "                                    str(horizon//2): tf.keras.layers.Dense(1, name='mid_step_forecasting'),\n",
    "                                    str(horizon-1): tf.keras.layers.Dense(1, name='last_step_forecasting')}\n",
    "\n",
    "                #self.quarter_forecasting = tf.keras.layers.Dense(4, activation='softmax', name='quarter_forecasting')\n",
    "                #self.month_forecasting = tf.keras.layers.Dense(12, activation='softmax', name='month_forecasting')\n",
    "                #self.day_of_week_forecasting = tf.keras.layers.Dense(7, activation='softmax', name='day_of_week_forecasting')\n",
    "\n",
    "                # self.mean_forecasting = tf.keras.layers.Dense(1, name='mean_forecasting')\n",
    "                \"\"\"\n",
    "                    Last Level tasks\n",
    "                \"\"\"\n",
    "                self.prediction_level_lstm_layer = tf.keras.layers.LSTM(d_model, return_sequences=False)\n",
    "                self.prediction_output = tf.keras.layers.Dense(horizon, name='prediction')\n",
    "\n",
    "            def get_mode(self, x, axis=1):\n",
    "                dt = x.dtype\n",
    "                # Shift input in case it has negative values\n",
    "                m = tf.math.reduce_min(x)\n",
    "                x2 = x - m\n",
    "                # minlength should not be necessary but may fail without it\n",
    "                # (reported here https://github.com/tensorflow/probability/issues/962)\n",
    "                c = tfp.stats.count_integers(x2, axis=axis, dtype=dt,\n",
    "                                             minlength=tf.math.reduce_max(x2) + 1)\n",
    "                # Find the values with largest counts\n",
    "                idx = tf.math.argmax(c, axis=0, output_type=dt)\n",
    "                # Get the modes by shifting by the subtracted minimum\n",
    "                modes = idx + m\n",
    "                # Get the number of counts\n",
    "                counts = tf.math.reduce_max(c, axis=0)\n",
    "\n",
    "                return modes\n",
    "\n",
    "            def gap_filling_task(self, inputs):\n",
    "\n",
    "                #Gap filling task\n",
    "                batch_indexes = tf.tile(tf.range(tf.shape(inputs)[0])[:, tf.newaxis, tf.newaxis], (1, self.window, 1))\n",
    "                head_indexes = tf.tile(tf.range(self.window)[tf.newaxis, :, tf.newaxis], (tf.shape(inputs)[0], 1, 1))\n",
    "                feat_index = tf.random.uniform((tf.shape(inputs)[0],window,1), minval=0, maxval=3, dtype=tf.int32)\n",
    "\n",
    "                idx = tf.squeeze(tf.stack(values=[batch_indexes, head_indexes, feat_index], axis=-1))\n",
    "                idx = tf.transpose(idx, perm=(1,2,0))    \n",
    "\n",
    "                gap_index =  tf.reshape(tf.transpose(tf.random.shuffle(idx), perm= (2, 0, 1))[:, :1, :], (-1, 3))\n",
    "\n",
    "                x_gap = tf.identity(inputs)\n",
    "                x_gap_updated = tf.tensor_scatter_nd_update(x_gap, indices = gap_index, updates = -tf.ones(gap_index.shape[0])*100)\n",
    "                x_gap_embedding = self.global_embedding_1(x_gap_updated)\n",
    "                x_gap_embedding = self.global_embedding_2(x_gap_embedding)\n",
    "                x_gap_embedding = self.gap_embedding_1(x_gap_embedding)\n",
    "                x_gap_embedding = self.gap_embedding_2(x_gap_embedding)\n",
    "                #gap_state = self.first_level_lstm_layer(x_gap_embedding)\n",
    "                gap_filling_output = self.gap_filling_output(x_gap_embedding)\n",
    "                #gap_filling_output = self.gap_filling_reshape(gap_filling_output_flat)\n",
    "                #gap_filling_true = tf.reshape(tf.gather_nd(x_gap, gap_index), (tf.shape(x_gap)[0], 1))\n",
    "                gap_loss = tf.keras.losses.MeanSquaredError()(gap_filling_output, x_gap)\n",
    "\n",
    "                self.add_metric(gap_loss, name='gap_loss', aggregation='mean')\n",
    "\n",
    "                return gap_filling_output, gap_loss\n",
    "\n",
    "            def noise_reduction_task(self, inputs):\n",
    "\n",
    "                input_copy = tf.identity(inputs)\n",
    "                x_noise = self.gaussian_noise(input_copy)\n",
    "\n",
    "                x_noise_embedding = self.global_embedding_1(x_noise)\n",
    "                x_noise_embedding = self.global_embedding_2(x_noise_embedding)\n",
    "                x_noise_embedding = self.gap_embedding_1(x_noise_embedding)\n",
    "                x_noise_embedding = self.gap_embedding_2(x_noise_embedding)\n",
    "                #noise_state = self.first_level_lstm_layer(x_noise_embedding)\n",
    "                #noise_state = self.noise_reduction_lstm_layer(noise_state)\n",
    "                noise_output = self.noise_reduction_output(x_noise_embedding)\n",
    "                #noise_output = self.noise_reduction_reshape(noise_output_flat)\n",
    "\n",
    "                noise_loss = tf.keras.losses.MeanSquaredError()(noise_output, input_copy)\n",
    "\n",
    "\n",
    "                self.add_metric(noise_loss, name='noise_loss', aggregation='mean')\n",
    "                \n",
    "\n",
    "                return noise_output, noise_loss\n",
    "\n",
    "            def smooth_forecast_task(self, second_level_state, labels, smooth=6):\n",
    "\n",
    "                smooth_state = self.smooth_lstm_layers[str(smooth)](second_level_state)\n",
    "                smooth_forecasting = self.smooth_layers[str(smooth)](smooth_state)\n",
    "                labels_smooth = tf.math.reduce_mean(tf.signal.frame(labels, smooth, 1, axis=1), axis=2)\n",
    "\n",
    "                smooth_loss = tf.keras.losses.MeanSquaredError()(smooth_forecasting, labels_smooth)\n",
    "\n",
    "                self.add_metric(smooth_loss, name=f'smooth_{smooth}_loss', aggregation='mean')\n",
    "                \n",
    "                return smooth_forecasting, smooth_state, smooth_loss\n",
    "\n",
    "            def one_step_forecast_task(self,second_level_state, labels, step=0):\n",
    "                step_state = self.one_step_forecast_lstm_layers[str(step)](second_level_state)\n",
    "                step_forecast = self.one_step_forecast[str(step)](step_state)\n",
    "                step_loss = tf.keras.losses.MeanSquaredError()(step_forecast, labels[:, step, :])\n",
    "\n",
    "                self.add_metric(step_loss, name=f'step_{step}_loss', aggregation='mean')\n",
    "                \n",
    "                return step_forecast, step_state, step_loss\n",
    "\n",
    "            def day_of_week_forecasting(self, second_level_state, labels_extra):\n",
    "                week_forecast = self.day_of_week_forecasting(second_level_state[:, -1, :])\n",
    "                week_loss = tf.keras.losses.MeanSquaredError()(week_forecast, self.get_mode(labels_extra[:, :, 0], axis=1))\n",
    "\n",
    "                self.add_metric(week_loss, name=f'week_forecasting_loss', aggregation='mean')\n",
    "\n",
    "            def day_of_week_sequence(self, first_level_state, inputs_extra):\n",
    "                # TODO: ONEHOT\n",
    "                week_forecast = self.day_of_week_sequence(first_level_state[:, -1, :])\n",
    "                week_loss = tf.keras.losses.MeanSquaredError()(week_forecast, self.get_mode(inputs_extra[:, :, 0], axis=1))\n",
    "\n",
    "                self.add_metric(week_loss, name=f'week_sequence_loss', aggregation='mean') \n",
    "\n",
    "            def quarter_forecasting(self, second_level_state, labels_extra):\n",
    "                quarter_forecast = self.quarter_forecasting(second_level_state[:, -1, :])\n",
    "                quarter_loss = tf.keras.losses.MeanSquaredError()(quarter_forecast, self.get_mode(labels_extra[:, :, 2], axis=1))\n",
    "\n",
    "                self.add_metric(quarter_loss, name=f'quarter_forecasting_loss', aggregation='mean')\n",
    "\n",
    "            def quarter_sequence(self, first_level_state, inputs_extra):\n",
    "                quarter_forecast = self.quarter_sequence(first_level_state[:, -1, :])\n",
    "                quarter_loss = tf.keras.losses.MeanSquaredError()(quarter_forecast, self.get_mode(inputs_extra[:, :, 2], axis=1))\n",
    "\n",
    "                self.add_metric(quarter_loss, name=f'quarter_sequence_loss', aggregation='mean') \n",
    "\n",
    "\n",
    "            def month_forecasting(self, second_level_state, labels_extra):\n",
    "                month_forecast = self.month_forecasting(second_level_state[:, -1, :])\n",
    "                month_loss = tf.keras.losses.MeanSquaredError()(month_forecast, self.get_mode(labels_extra[:, :, 1], axis=1))\n",
    "\n",
    "                self.add_metric(month_loss, name=f'month_forecasting_loss', aggregation='mean')\n",
    "\n",
    "\n",
    "            def month_sequence(self, first_level_state, inputs_extra):\n",
    "                month_forecast = self.month_sequence(first_level_state[:, -1, :])\n",
    "                month_loss = tf.keras.losses.MeanSquaredError()(month_forecast, self.get_mode(inputs_extra[:, :, 1], axis=1))\n",
    "\n",
    "                self.add_metric(month_loss, name=f'month_sequence_loss', aggregation='mean')                \n",
    "\n",
    "            def swap_task(self, inputs):\n",
    "\n",
    "                x_swap = tf.identity(inputs)\n",
    "\n",
    "                idx = tf.tile([tf.range(0, window)], (tf.shape(x_swap)[0], 1))\n",
    "\n",
    "                swap_index = tf.random.uniform((tf.shape(x_swap)[0], 1), 0, len(self.swap_combinations), dtype=tf.int64)\n",
    "\n",
    "                swap_index_onehot = tf.one_hot(swap_index, len(self.swap_combinations))\n",
    "\n",
    "                swap = tf.gather(tf.convert_to_tensor(self.swap_combinations), swap_index)\n",
    "\n",
    "                swap_numpy = swap.numpy()\n",
    "                idx_numpy = idx.numpy()\n",
    "                idx_numpy = np.concatenate((idx_numpy.reshape(idx_numpy.shape[0], 4, -1)[np.arange(idx_numpy.shape[0]), swap_numpy[..., 0].squeeze()],\n",
    "                            idx_numpy.reshape(idx_numpy.shape[0], 4, -1)[np.arange(idx_numpy.shape[0]), swap_numpy[..., 1].squeeze()],\n",
    "                            idx_numpy.reshape(idx_numpy.shape[0], 4, -1)[np.arange(idx_numpy.shape[0]), swap_numpy[..., 2].squeeze()],\n",
    "                            idx_numpy.reshape(idx_numpy.shape[0], 4, -1)[np.arange(idx_numpy.shape[0]), swap_numpy[..., 3].squeeze()]), axis=1)\n",
    "\n",
    "                idx = tf.convert_to_tensor(idx_numpy)\n",
    "                x_swap = tf.gather(x_swap, idx, axis=1, batch_dims=1)\n",
    "\n",
    "                x_swap_embedding = self.embedding(x_swap)\n",
    "                swap_state = self.first_level_lstm_layer(x_swap_embedding)\n",
    "                swap_state = self.swap_lstm_layer(swap_state)\n",
    "                swap_output = self.swap_output(swap_state)\n",
    "\n",
    "                swap_loss = tf.keras.losses.CategoricalCrossentropy()(tf.squeeze(swap_index_onehot), swap_output)\n",
    "\n",
    "                self.add_metric(swap_loss, name='swap_loss', aggregation='mean')\n",
    "\n",
    "                return swap_output, swap_state\n",
    "\n",
    "\n",
    "            def call(self, inputs):\n",
    "                labels, labels_extra = inputs[1]\n",
    "                inputs, inputs_extra = inputs[0]\n",
    "\n",
    "                x = self.global_embedding_1(inputs)\n",
    "                x = self.global_embedding_2(x)\n",
    "                #first_level_state = self.first_level_lstm_layer(x)\n",
    "\n",
    "                # First level\n",
    "                gap_filling_outputs, gap_loss = self.gap_filling_task(inputs)\n",
    "                noise_reduction_outputs, noise_loss = self.noise_reduction_task(inputs)\n",
    "                #swap_output, swap_state = self.swap_task(inputs)\n",
    "\n",
    "                #Second level\n",
    "                second_level_state = self.second_level_lstm_layer(x)\n",
    "                smooth_forecasting_8, smooth_state_8, smooth_loss_8 = self.smooth_forecast_task(second_level_state, labels, 8)\n",
    "                smooth_forecasting_6, smooth_state_6, smooth_loss_6 = self.smooth_forecast_task(second_level_state, labels, 6)\n",
    "                smooth_forecasting_3, smooth_state_3, smooth_loss_3 = self.smooth_forecast_task(second_level_state, labels, 3)\n",
    "\n",
    "                first_step_forecast, first_step_state, first_step_loss = self.one_step_forecast_task(second_level_state, labels, 0)\n",
    "                mid_step_forecast, mid_step_state, mid_step_loss = self.one_step_forecast_task(second_level_state, labels, self.horizon//2)\n",
    "                last_step_forecast, last_step_state, last_step_loss = self.one_step_forecast_task(second_level_state, labels, self.horizon-1)\n",
    "                \n",
    "                self.add_loss(tf.reduce_mean([gap_loss, noise_loss, \n",
    "                                                             smooth_loss_8, smooth_loss_6,\n",
    "                                                             smooth_loss_3, first_step_loss,\n",
    "                                                             mid_step_loss, last_step_loss]))\n",
    "                # Last level\n",
    "                prediction_level_state = self.prediction_level_lstm_layer(second_level_state)\n",
    "                prediction_state = tf.keras.layers.concatenate([prediction_level_state, last_step_state, mid_step_state, \n",
    "                                         first_step_state, smooth_state_3, smooth_state_6, smooth_state_8])\n",
    "                \n",
    "                prediction_output = self.prediction_output(prediction_state)\n",
    "\n",
    "                return prediction_output\n",
    "\n",
    "\n",
    "        model = MultitaskHLNet(d_model, n_features, window, horizon, batch_size)\n",
    "\n",
    "        loss = 'mse'\n",
    "        \n",
    "    elif model_type == 'multitaskhlnet_taskslstm_firstlevelcnn_gradsprojected':\n",
    "    #pred_index = -1\n",
    "\n",
    "        class MultitaskHLNet(tf.keras.Model):\n",
    "\n",
    "            def __init__(self, d_model, n_features, window, horizon, batch_size, grad_smooth_alpha, lamb1, lamb2, mode, overall):\n",
    "                super(MultitaskHLNet, self).__init__()\n",
    "                self.d_model = d_model\n",
    "                self.n_features = n_features\n",
    "                self.window = window\n",
    "                self.horizon = horizon\n",
    "                self.batch_size = batch_size\n",
    "                self.main_grad_first_level_average = None\n",
    "                self.main_grad_second_level_average = None\n",
    "                self.first_level_aux_grad_average = None\n",
    "                self.grad_smooth_alpha = grad_smooth_alpha\n",
    "                self.lamb1 = lamb1\n",
    "                self.lamb2 = lamb2\n",
    "                self.mode = mode\n",
    "                self.overall = overall\n",
    "                \n",
    "                self.loss_weigts = tf.Variable(tf.ones([8]))\n",
    "                \n",
    "                self.global_embedding_1 = tf.keras.layers.Conv1D(d_model//2, 3, activation='relu', input_shape=(window, 3), padding='causal')\n",
    "                self.global_embedding_2 = tf.keras.layers.Conv1D(d_model, 3, activation='relu', padding='causal')\n",
    "                \n",
    "                self.global_first_level_layers = [0,1]\n",
    "                \"\"\"\n",
    "                    First Level tasks - Sequence featurization\n",
    "                \"\"\"\n",
    "                self.decode_cnn_1 = tf.keras.layers.Conv1DTranspose(d_model, 3, activation='relu', padding='same')\n",
    "                self.decode_cnn_2 = tf.keras.layers.Conv1DTranspose(d_model//2, 3, activation='relu', padding='same')\n",
    "                self.gap_filling_output = tf.keras.layers.Dense(n_features)\n",
    "\n",
    "                self.gaussian_noise = tf.keras.layers.GaussianNoise(0.01)\n",
    "                self.noise_reduction_output = tf.keras.layers.Dense(n_features)\n",
    "\n",
    "                #self.swap_combinations = list(itertools.permutations(np.arange(0, 4), 4))\n",
    "                #self.swap_lstm_layer = tf.keras.layers.LSTM(d_model, name='swap_lstm', activation='relu')\n",
    "                #self.swap_output = tf.keras.layers.Dense(len(self.swap_combinations), name='swap_task', activation='softmax')\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                    Second Level tasks - Forecasting helpers\n",
    "                \"\"\"\n",
    "                self.second_level_lstm_layer = tf.keras.layers.LSTM(d_model, return_sequences=True)\n",
    "\n",
    "                self.smooth_lstm_layers = {'8': tf.keras.layers.LSTM(d_model, name='smooth_forecasting_8_lstm', activation='relu'),\n",
    "                                '6':  tf.keras.layers.LSTM(d_model, name='smooth_forecasting_6_lstm', activation='relu'),\n",
    "                                '3':  tf.keras.layers.LSTM(d_model, name='smooth_forecasting_3_lstm', activation='relu')}\n",
    "\n",
    "                self.smooth_layers = {'8': tf.keras.layers.Dense(horizon-8+1, name='smooth_forecasting_8'),\n",
    "                                '6': tf.keras.layers.Dense(horizon-6+1, name='smooth_forecasting_6'),\n",
    "                                '3': tf.keras.layers.Dense(horizon-3+1, name='smooth_forecasting_3')}\n",
    "\n",
    "\n",
    "                self.one_step_forecast_lstm_layers = {'0': tf.keras.layers.LSTM(d_model, name='next_step_forecasting_lstm', activation='relu'),\n",
    "                                str(horizon//2):  tf.keras.layers.LSTM(d_model, name='mid_step_forecasting_lstm', activation='relu'),\n",
    "                                str(horizon-1):  tf.keras.layers.LSTM(d_model, name='last_step_forecasting_lstm', activation='relu')}\n",
    "\n",
    "                self.one_step_forecast = {'0': tf.keras.layers.Dense(1, name='next_step_forecasting'),\n",
    "                                    str(horizon//2): tf.keras.layers.Dense(1, name='mid_step_forecasting'),\n",
    "                                    str(horizon-1): tf.keras.layers.Dense(1, name='last_step_forecasting')}\n",
    "                self.global_second_level_layers = [7]\n",
    "                \"\"\"\n",
    "                    Last Level tasks\n",
    "                \"\"\"\n",
    "                self.prediction_level_lstm_layer = tf.keras.layers.LSTM(d_model, return_sequences=False)\n",
    "                self.prediction_output = tf.keras.layers.Dense(horizon, name='prediction')\n",
    "\n",
    "            def get_mode(self, x, axis=1):\n",
    "                dt = x.dtype\n",
    "                # Shift input in case it has negative values\n",
    "                m = tf.math.reduce_min(x)\n",
    "                x2 = x - m\n",
    "                # minlength should not be necessary but may fail without it\n",
    "                # (reported here https://github.com/tensorflow/probability/issues/962)\n",
    "                c = tfp.stats.count_integers(x2, axis=axis, dtype=dt,\n",
    "                                             minlength=tf.math.reduce_max(x2) + 1)\n",
    "                # Find the values with largest counts\n",
    "                idx = tf.math.argmax(c, axis=0, output_type=dt)\n",
    "                # Get the modes by shifting by the subtracted minimum\n",
    "                modes = idx + m\n",
    "                # Get the number of counts\n",
    "                counts = tf.math.reduce_max(c, axis=0)\n",
    "\n",
    "                return modes\n",
    "\n",
    "            def gap_filling_task(self, inputs):\n",
    "\n",
    "                #Gap filling task\n",
    "                batch_indexes = tf.tile(tf.range(tf.shape(inputs)[0])[:, tf.newaxis, tf.newaxis], (1, self.window, 1))\n",
    "                head_indexes = tf.tile(tf.range(self.window)[tf.newaxis, :, tf.newaxis], (tf.shape(inputs)[0], 1, 1))\n",
    "                feat_index = tf.random.uniform((tf.shape(inputs)[0],window,1), minval=0, maxval=3, dtype=tf.int32)\n",
    "\n",
    "                idx = tf.squeeze(tf.stack(values=[batch_indexes, head_indexes, feat_index], axis=-1))\n",
    "                idx = tf.transpose(idx, perm=(1,2,0))    \n",
    "\n",
    "                gap_index =  tf.reshape(tf.transpose(tf.random.shuffle(idx), perm= (2, 0, 1))[:, :1, :], (-1, 3))\n",
    "\n",
    "                x_gap = tf.identity(inputs)\n",
    "                x_gap_updated = tf.tensor_scatter_nd_update(x_gap, indices = gap_index, updates = -tf.ones(gap_index.shape[0])*100)\n",
    "                x_gap_embedding = self.global_embedding_1(x_gap_updated)\n",
    "                x_gap_embedding = self.global_embedding_2(x_gap_embedding)\n",
    "                x_gap_embedding = self.decode_cnn_1(x_gap_embedding)\n",
    "                x_gap_embedding = self.decode_cnn_2(x_gap_embedding)\n",
    "                #gap_state = self.first_level_lstm_layer(x_gap_embedding)\n",
    "                gap_filling_output = self.gap_filling_output(x_gap_embedding)\n",
    "                #gap_filling_output = self.gap_filling_reshape(gap_filling_output_flat)\n",
    "                #gap_filling_true = tf.reshape(tf.gather_nd(x_gap, gap_index), (tf.shape(x_gap)[0], 1))\n",
    "                gap_loss = tf.keras.losses.MeanSquaredError()(gap_filling_output, x_gap)\n",
    "\n",
    "                self.add_metric(gap_loss, name='gap_loss', aggregation='mean')\n",
    "                \n",
    "\n",
    "                return gap_filling_output, gap_loss\n",
    "\n",
    "            def noise_reduction_task(self, inputs):\n",
    "\n",
    "                input_copy = tf.identity(inputs)\n",
    "                x_noise = self.gaussian_noise(input_copy)\n",
    "\n",
    "                x_noise_embedding = self.global_embedding_1(x_noise)\n",
    "                x_noise_embedding = self.global_embedding_2(x_noise_embedding)\n",
    "                x_noise_embedding = self.decode_cnn_1(x_noise_embedding)\n",
    "                x_noise_embedding = self.decode_cnn_2(x_noise_embedding)\n",
    "                #noise_state = self.first_level_lstm_layer(x_noise_embedding)\n",
    "                #noise_state = self.noise_reduction_lstm_layer(noise_state)\n",
    "                noise_output = self.noise_reduction_output(x_noise_embedding)\n",
    "                #noise_output = self.noise_reduction_reshape(noise_output_flat)\n",
    "\n",
    "                noise_loss = tf.keras.losses.MeanSquaredError()(noise_output, input_copy)\n",
    "\n",
    "                self.add_metric(noise_loss, name='noise_loss', aggregation='mean')\n",
    "                \n",
    "\n",
    "                return noise_output, noise_loss\n",
    "\n",
    "            def smooth_forecast_task(self, second_level_state, labels, smooth=6):\n",
    "\n",
    "                smooth_state = self.smooth_lstm_layers[str(smooth)](second_level_state)\n",
    "                smooth_forecasting = self.smooth_layers[str(smooth)](smooth_state)\n",
    "                labels_smooth = tf.math.reduce_mean(tf.signal.frame(labels, smooth, 1, axis=1), axis=2)\n",
    "\n",
    "                smooth_loss = tf.keras.losses.MeanSquaredError()(smooth_forecasting, labels_smooth)\n",
    "\n",
    "                self.add_metric(smooth_loss, name=f'smooth_{smooth}_loss', aggregation='mean')\n",
    "                \n",
    "\n",
    "                return smooth_forecasting, smooth_state, smooth_loss\n",
    "\n",
    "            def one_step_forecast_task(self,second_level_state, labels, step=0):\n",
    "                step_state = self.one_step_forecast_lstm_layers[str(step)](second_level_state)\n",
    "                step_forecast = self.one_step_forecast[str(step)](step_state)\n",
    "                step_loss = tf.keras.losses.MeanSquaredError()(step_forecast, labels[:, step, :])\n",
    "\n",
    "                self.add_metric(step_loss, name=f'step_{step}_loss', aggregation='mean')\n",
    "                \n",
    "                \n",
    "                return step_forecast, step_state, step_loss\n",
    "\n",
    "            def day_of_week_forecasting(self, second_level_state, labels_extra):\n",
    "                week_forecast = self.day_of_week_forecasting(second_level_state[:, -1, :])\n",
    "                week_loss = tf.keras.losses.MeanSquaredError()(week_forecast, self.get_mode(labels_extra[:, :, 0], axis=1))\n",
    "\n",
    "                self.add_metric(week_loss, name=f'week_forecasting_loss', aggregation='mean')\n",
    "\n",
    "            def day_of_week_sequence(self, first_level_state, inputs_extra):\n",
    "                # TODO: ONEHOT\n",
    "                week_forecast = self.day_of_week_sequence(first_level_state[:, -1, :])\n",
    "                week_loss = tf.keras.losses.MeanSquaredError()(week_forecast, self.get_mode(inputs_extra[:, :, 0], axis=1))\n",
    "\n",
    "                self.add_metric(week_loss, name=f'week_sequence_loss', aggregation='mean') \n",
    "\n",
    "            def quarter_forecasting(self, second_level_state, labels_extra):\n",
    "                quarter_forecast = self.quarter_forecasting(second_level_state[:, -1, :])\n",
    "                quarter_loss = tf.keras.losses.MeanSquaredError()(quarter_forecast, self.get_mode(labels_extra[:, :, 2], axis=1))\n",
    "\n",
    "                self.add_metric(quarter_loss, name=f'quarter_forecasting_loss', aggregation='mean')\n",
    "\n",
    "            def quarter_sequence(self, first_level_state, inputs_extra):\n",
    "                quarter_forecast = self.quarter_sequence(first_level_state[:, -1, :])\n",
    "                quarter_loss = tf.keras.losses.MeanSquaredError()(quarter_forecast, self.get_mode(inputs_extra[:, :, 2], axis=1))\n",
    "\n",
    "                self.add_metric(quarter_loss, name=f'quarter_sequence_loss', aggregation='mean') \n",
    "\n",
    "\n",
    "            def month_forecasting(self, second_level_state, labels_extra):\n",
    "                month_forecast = self.month_forecasting(second_level_state[:, -1, :])\n",
    "                month_loss = tf.keras.losses.MeanSquaredError()(month_forecast, self.get_mode(labels_extra[:, :, 1], axis=1))\n",
    "\n",
    "                self.add_metric(month_loss, name=f'month_forecasting_loss', aggregation='mean')\n",
    "\n",
    "\n",
    "            def month_sequence(self, first_level_state, inputs_extra):\n",
    "                month_forecast = self.month_sequence(first_level_state[:, -1, :])\n",
    "                month_loss = tf.keras.losses.MeanSquaredError()(month_forecast, self.get_mode(inputs_extra[:, :, 1], axis=1))\n",
    "\n",
    "                self.add_metric(month_loss, name=f'month_sequence_loss', aggregation='mean')                \n",
    "\n",
    "            def swap_task(self, inputs):\n",
    "\n",
    "                x_swap = tf.identity(inputs)\n",
    "\n",
    "                idx = tf.tile([tf.range(0, window)], (tf.shape(x_swap)[0], 1))\n",
    "\n",
    "                swap_index = tf.random.uniform((tf.shape(x_swap)[0], 1), 0, len(self.swap_combinations), dtype=tf.int64)\n",
    "\n",
    "                swap_index_onehot = tf.one_hot(swap_index, len(self.swap_combinations))\n",
    "\n",
    "                swap = tf.gather(tf.convert_to_tensor(self.swap_combinations), swap_index)\n",
    "\n",
    "                swap_numpy = swap.numpy()\n",
    "                idx_numpy = idx.numpy()\n",
    "                idx_numpy = np.concatenate((idx_numpy.reshape(idx_numpy.shape[0], 4, -1)[np.arange(idx_numpy.shape[0]), swap_numpy[..., 0].squeeze()],\n",
    "                            idx_numpy.reshape(idx_numpy.shape[0], 4, -1)[np.arange(idx_numpy.shape[0]), swap_numpy[..., 1].squeeze()],\n",
    "                            idx_numpy.reshape(idx_numpy.shape[0], 4, -1)[np.arange(idx_numpy.shape[0]), swap_numpy[..., 2].squeeze()],\n",
    "                            idx_numpy.reshape(idx_numpy.shape[0], 4, -1)[np.arange(idx_numpy.shape[0]), swap_numpy[..., 3].squeeze()]), axis=1)\n",
    "\n",
    "                idx = tf.convert_to_tensor(idx_numpy)\n",
    "                x_swap = tf.gather(x_swap, idx, axis=1, batch_dims=1)\n",
    "\n",
    "                x_swap_embedding = self.embedding(x_swap)\n",
    "                swap_state = self.first_level_lstm_layer(x_swap_embedding)\n",
    "                swap_state = self.swap_lstm_layer(swap_state)\n",
    "                swap_output = self.swap_output(swap_state)\n",
    "\n",
    "                swap_loss = tf.keras.losses.CategoricalCrossentropy()(tf.squeeze(swap_index_onehot), swap_output)\n",
    "\n",
    "                self.add_metric(swap_loss, name='swap_loss', aggregation='mean')\n",
    "\n",
    "                return swap_output, swap_state\n",
    "\n",
    "            def train_step(self, data):\n",
    "                inputs, labels = data\n",
    "                gradients = []\n",
    "                \n",
    "                with tf.GradientTape(persistent=True) as tape:\n",
    "                    prediction, losses = self(inputs, True)\n",
    "                    loss = self.compiled_loss(labels, prediction, regularization_losses=self.losses)\n",
    "\n",
    "                self.compiled_metrics.update_state(labels, prediction)\n",
    "                \n",
    "                model_layers = np.array(self.layers)\n",
    "\n",
    "                # First level loss\n",
    "                first_level_trainable_weights = [w for layer in model_layers[model.global_first_level_layers] for w in layer.trainable_weights]    \n",
    "                main_grad_first_level = tape.gradient(loss, first_level_trainable_weights)\n",
    "                self.main_grad_first_level_average = update_smooth_grad(main_grad_first_level, self.main_grad_first_level_average, grad_smooth_alpha)\n",
    "\n",
    "                first_level_gradients = []\n",
    "                for l in losses[:2]:\n",
    "\n",
    "                    aux_grad = tape.gradient(l, first_level_trainable_weights)\n",
    "                    aux_grad = combined_grads(main_grad_first_level, self.main_grad_first_level_average, aux_grad, mode, overall, lamb1)\n",
    "\n",
    "                    first_level_gradients.append(aux_grad)\n",
    "\n",
    "                    gradients.extend(list(zip(aux_grad, first_level_trainable_weights)))\n",
    "\n",
    "                first_level_aux_grad = combined_grads(first_level_gradients[0], None, first_level_gradients[1], 'Multitask', overall, 1)\n",
    "                self.first_level_aux_grad_average = update_smooth_grad(first_level_aux_grad, self.first_level_aux_grad_average, grad_smooth_alpha)\n",
    "\n",
    "                #Second level loss\n",
    "                first_level_trainable_weights = [w for layer in model_layers[self.global_first_level_layers] for w in layer.trainable_weights]    \n",
    "\n",
    "                for l in losses[2:]:\n",
    "                    aux_grad = tape.gradient(l, first_level_trainable_weights)\n",
    "                    aux_grad = combined_grads(first_level_aux_grad, self.first_level_aux_grad_average, aux_grad, mode, overall, lamb2)\n",
    "                    gradients.extend(list(zip(aux_grad, first_level_trainable_weights)))\n",
    "\n",
    "                second_level_trainable_weights = [w for layer in model_layers[self.global_second_level_layers] for w in layer.trainable_weights]    \n",
    "                main_grad_second_level = tape.gradient(loss, second_level_trainable_weights)\n",
    "                self.main_grad_second_level_average = update_smooth_grad(main_grad_second_level, self.main_grad_second_level_average, grad_smooth_alpha)\n",
    "\n",
    "                for l in losses[2:]:\n",
    "                    aux_grad = tape.gradient(l, second_level_trainable_weights)\n",
    "                    aux_grad = combined_grads(main_grad_second_level, self.main_grad_second_level_average, aux_grad, mode, overall, lamb2)\n",
    "\n",
    "                    gradients.extend(list(zip(aux_grad, second_level_trainable_weights)))\n",
    "\n",
    "                # All losses\n",
    "                rest_layers = sum([self.global_second_level_layers, self.global_first_level_layers], [])\n",
    "\n",
    "                mask = np.ones(len(model_layers), bool)\n",
    "                mask[rest_layers] = False\n",
    "                all_level_trainable_weights = [w for layer in model_layers[mask] for w in layer.trainable_weights]    \n",
    "                main_grad_rest_layers = tape.gradient(loss, all_level_trainable_weights)\n",
    "\n",
    "                for l in losses:\n",
    "                    grad = tape.gradient(l, all_level_trainable_weights)\n",
    "                    gradients.extend(list(zip(grad, all_level_trainable_weights)))\n",
    "                gradients.extend(list(zip(main_grad_rest_layers, all_level_trainable_weights)))\n",
    "                #Apply all gradients\n",
    "                self.optimizer.apply_gradients(gradients)\n",
    "\n",
    "                return {m.name: m.result() for m in self.metrics}\n",
    "            \n",
    "            def call(self, inputs, training):\n",
    "                labels, labels_extra = inputs[1]\n",
    "                inputs, inputs_extra = inputs[0]\n",
    "\n",
    "                x = self.global_embedding_1(inputs)\n",
    "                x = self.global_embedding_2(x)\n",
    "                #first_level_state = self.first_level_lstm_layer(x)\n",
    "\n",
    "                # First level\n",
    "                gap_filling_outputs, gap_loss = self.gap_filling_task(inputs)\n",
    "                noise_reduction_outputs, noise_loss = self.noise_reduction_task(inputs)\n",
    "                #swap_output, swap_state = self.swap_task(inputs)\n",
    "\n",
    "                #Second level\n",
    "                second_level_state = self.second_level_lstm_layer(x)\n",
    "                smooth_forecasting_8, smooth_state_8, smooth_loss_8 = self.smooth_forecast_task(second_level_state, labels, 8)\n",
    "                smooth_forecasting_6, smooth_state_6, smooth_loss_6 = self.smooth_forecast_task(second_level_state, labels, 6)\n",
    "                smooth_forecasting_3, smooth_state_3, smooth_loss_3 = self.smooth_forecast_task(second_level_state, labels, 3)\n",
    "\n",
    "                first_step_forecast, first_step_state, first_step_loss = self.one_step_forecast_task(second_level_state, labels, 0)\n",
    "                mid_step_forecast, mid_step_state, mid_step_loss = self.one_step_forecast_task(second_level_state, labels, self.horizon//2)\n",
    "                last_step_forecast, last_step_state, last_step_loss = self.one_step_forecast_task(second_level_state, labels, self.horizon-1)\n",
    "                \n",
    "                losses = [gap_loss, noise_loss, smooth_loss_8, smooth_loss_6,\n",
    "                        smooth_loss_3, first_step_loss, mid_step_loss, last_step_loss]\n",
    "                \n",
    "                \"\"\"tasks_loss = 0\n",
    "                for i, loss in enumerate(losses):\n",
    "                    losses[i] = (0.5/self.loss_weigts[i]**2)*loss + tf.math.log(1+self.loss_weigts[i]**2)\"\"\"\n",
    "                    \n",
    "                #self.add_loss(tasks_loss)\n",
    "                # Last level\n",
    "                prediction_level_state = self.prediction_level_lstm_layer(second_level_state)\n",
    "                prediction_state = tf.keras.layers.concatenate([prediction_level_state, last_step_state, mid_step_state, \n",
    "                                         first_step_state, smooth_state_3, smooth_state_6, smooth_state_8])\n",
    "                \n",
    "                prediction_output = self.prediction_output(prediction_state)\n",
    "                \n",
    "                if training:\n",
    "                    return prediction_output, losses\n",
    "                else:\n",
    "                    return prediction_output\n",
    "\n",
    "\n",
    "        model = MultitaskHLNet(d_model, n_features, window, horizon, batch_size, grad_smooth_alpha, lamb1, lamb2, mode, overall)\n",
    "\n",
    "        loss = 'mse'\n",
    "    elif model_type == 'multitaskhlnet_taskslstm_firstlevelcnn_gradsprojected_nosmooth':\n",
    "    #pred_index = -1\n",
    "\n",
    "        class MultitaskHLNet(tf.keras.Model):\n",
    "\n",
    "            def __init__(self, d_model, n_features, window, horizon, batch_size, grad_smooth_alpha, lamb1, lamb2, mode, overall):\n",
    "                super(MultitaskHLNet, self).__init__()\n",
    "                self.d_model = d_model\n",
    "                self.n_features = n_features\n",
    "                self.window = window\n",
    "                self.horizon = horizon\n",
    "                self.batch_size = batch_size\n",
    "                self.main_grad_first_level_average = None\n",
    "                self.main_grad_second_level_average = None\n",
    "                self.first_level_aux_grad_average = None\n",
    "                self.grad_smooth_alpha = grad_smooth_alpha\n",
    "                self.lamb1 = lamb1\n",
    "                self.lamb2 = lamb2\n",
    "                self.mode = mode\n",
    "                self.overall = overall\n",
    "                \n",
    "                self.loss_weigts = tf.Variable(tf.ones([8]))\n",
    "                \n",
    "                self.global_embedding_1 = tf.keras.layers.Conv1D(d_model//2, 3, activation='relu', input_shape=(window, 3), padding='causal')\n",
    "                self.global_embedding_2 = tf.keras.layers.Conv1D(d_model, 3, activation='relu', padding='causal')\n",
    "                \n",
    "                self.global_first_level_layers = [0,1]\n",
    "                \"\"\"\n",
    "                    First Level tasks - Sequence featurization\n",
    "                \"\"\"\n",
    "                self.decode_cnn_1 = tf.keras.layers.Conv1DTranspose(d_model, 3, activation='relu', padding='same')\n",
    "                self.decode_cnn_2 = tf.keras.layers.Conv1DTranspose(d_model//2, 3, activation='relu', padding='same')\n",
    "                self.gap_filling_output = tf.keras.layers.Dense(n_features)\n",
    "\n",
    "                self.gaussian_noise = tf.keras.layers.GaussianNoise(0.01)\n",
    "                self.noise_reduction_output = tf.keras.layers.Dense(n_features)\n",
    "\n",
    "                #self.swap_combinations = list(itertools.permutations(np.arange(0, 4), 4))\n",
    "                #self.swap_lstm_layer = tf.keras.layers.LSTM(d_model, name='swap_lstm', activation='relu')\n",
    "                #self.swap_output = tf.keras.layers.Dense(len(self.swap_combinations), name='swap_task', activation='softmax')\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                    Second Level tasks - Forecasting helpers\n",
    "                \"\"\"\n",
    "                self.second_level_lstm_layer = tf.keras.layers.LSTM(d_model, return_sequences=True)\n",
    "\n",
    "                self.smooth_lstm_layers = {'8': tf.keras.layers.LSTM(d_model, name='smooth_forecasting_8_lstm', activation='relu'),\n",
    "                                '6':  tf.keras.layers.LSTM(d_model, name='smooth_forecasting_6_lstm', activation='relu'),\n",
    "                                '3':  tf.keras.layers.LSTM(d_model, name='smooth_forecasting_3_lstm', activation='relu')}\n",
    "\n",
    "                self.smooth_layers = {'8': tf.keras.layers.Dense(horizon-8+1, name='smooth_forecasting_8'),\n",
    "                                '6': tf.keras.layers.Dense(horizon-6+1, name='smooth_forecasting_6'),\n",
    "                                '3': tf.keras.layers.Dense(horizon-3+1, name='smooth_forecasting_3')}\n",
    "\n",
    "\n",
    "                self.one_step_forecast_lstm_layers = {'0': tf.keras.layers.LSTM(d_model, name='next_step_forecasting_lstm', activation='relu'),\n",
    "                                str(horizon//2):  tf.keras.layers.LSTM(d_model, name='mid_step_forecasting_lstm', activation='relu'),\n",
    "                                str(horizon-1):  tf.keras.layers.LSTM(d_model, name='last_step_forecasting_lstm', activation='relu')}\n",
    "\n",
    "                self.one_step_forecast = {'0': tf.keras.layers.Dense(1, name='next_step_forecasting'),\n",
    "                                    str(horizon//2): tf.keras.layers.Dense(1, name='mid_step_forecasting'),\n",
    "                                    str(horizon-1): tf.keras.layers.Dense(1, name='last_step_forecasting')}\n",
    "                self.global_second_level_layers = [7]\n",
    "                \"\"\"\n",
    "                    Last Level tasks\n",
    "                \"\"\"\n",
    "                self.prediction_level_lstm_layer = tf.keras.layers.LSTM(d_model, return_sequences=False)\n",
    "                self.prediction_output = tf.keras.layers.Dense(horizon, name='prediction')\n",
    "\n",
    "            def get_mode(self, x, axis=1):\n",
    "                dt = x.dtype\n",
    "                # Shift input in case it has negative values\n",
    "                m = tf.math.reduce_min(x)\n",
    "                x2 = x - m\n",
    "                # minlength should not be necessary but may fail without it\n",
    "                # (reported here https://github.com/tensorflow/probability/issues/962)\n",
    "                c = tfp.stats.count_integers(x2, axis=axis, dtype=dt,\n",
    "                                             minlength=tf.math.reduce_max(x2) + 1)\n",
    "                # Find the values with largest counts\n",
    "                idx = tf.math.argmax(c, axis=0, output_type=dt)\n",
    "                # Get the modes by shifting by the subtracted minimum\n",
    "                modes = idx + m\n",
    "                # Get the number of counts\n",
    "                counts = tf.math.reduce_max(c, axis=0)\n",
    "\n",
    "                return modes\n",
    "\n",
    "            def gap_filling_task(self, inputs):\n",
    "\n",
    "                #Gap filling task\n",
    "                batch_indexes = tf.tile(tf.range(tf.shape(inputs)[0])[:, tf.newaxis, tf.newaxis], (1, self.window, 1))\n",
    "                head_indexes = tf.tile(tf.range(self.window)[tf.newaxis, :, tf.newaxis], (tf.shape(inputs)[0], 1, 1))\n",
    "                feat_index = tf.random.uniform((tf.shape(inputs)[0],window,1), minval=0, maxval=3, dtype=tf.int32)\n",
    "\n",
    "                idx = tf.squeeze(tf.stack(values=[batch_indexes, head_indexes, feat_index], axis=-1))\n",
    "                idx = tf.transpose(idx, perm=(1,2,0))    \n",
    "\n",
    "                gap_index =  tf.reshape(tf.transpose(tf.random.shuffle(idx), perm= (2, 0, 1))[:, :1, :], (-1, 3))\n",
    "\n",
    "                x_gap = tf.identity(inputs)\n",
    "                x_gap_updated = tf.tensor_scatter_nd_update(x_gap, indices = gap_index, updates = -tf.ones(gap_index.shape[0])*100)\n",
    "                x_gap_embedding = self.global_embedding_1(x_gap_updated)\n",
    "                x_gap_embedding = self.global_embedding_2(x_gap_embedding)\n",
    "                x_gap_embedding = self.decode_cnn_1(x_gap_embedding)\n",
    "                x_gap_embedding = self.decode_cnn_2(x_gap_embedding)\n",
    "                #gap_state = self.first_level_lstm_layer(x_gap_embedding)\n",
    "                gap_filling_output = self.gap_filling_output(x_gap_embedding)\n",
    "                #gap_filling_output = self.gap_filling_reshape(gap_filling_output_flat)\n",
    "                #gap_filling_true = tf.reshape(tf.gather_nd(x_gap, gap_index), (tf.shape(x_gap)[0], 1))\n",
    "                gap_loss = tf.keras.losses.MeanSquaredError()(gap_filling_output, x_gap)\n",
    "\n",
    "                self.add_metric(gap_loss, name='gap_loss', aggregation='mean')\n",
    "                \n",
    "\n",
    "                return gap_filling_output, gap_loss\n",
    "\n",
    "            def noise_reduction_task(self, inputs):\n",
    "\n",
    "                input_copy = tf.identity(inputs)\n",
    "                x_noise = self.gaussian_noise(input_copy)\n",
    "\n",
    "                x_noise_embedding = self.global_embedding_1(x_noise)\n",
    "                x_noise_embedding = self.global_embedding_2(x_noise_embedding)\n",
    "                x_noise_embedding = self.decode_cnn_1(x_noise_embedding)\n",
    "                x_noise_embedding = self.decode_cnn_2(x_noise_embedding)\n",
    "                #noise_state = self.first_level_lstm_layer(x_noise_embedding)\n",
    "                #noise_state = self.noise_reduction_lstm_layer(noise_state)\n",
    "                noise_output = self.noise_reduction_output(x_noise_embedding)\n",
    "                #noise_output = self.noise_reduction_reshape(noise_output_flat)\n",
    "\n",
    "                noise_loss = tf.keras.losses.MeanSquaredError()(noise_output, input_copy)\n",
    "\n",
    "                self.add_metric(noise_loss, name='noise_loss', aggregation='mean')\n",
    "                \n",
    "\n",
    "                return noise_output, noise_loss\n",
    "\n",
    "            def smooth_forecast_task(self, second_level_state, labels, smooth=6):\n",
    "\n",
    "                smooth_state = self.smooth_lstm_layers[str(smooth)](second_level_state)\n",
    "                smooth_forecasting = self.smooth_layers[str(smooth)](smooth_state)\n",
    "                labels_smooth = tf.math.reduce_mean(tf.signal.frame(labels, smooth, 1, axis=1), axis=2)\n",
    "\n",
    "                smooth_loss = tf.keras.losses.MeanSquaredError()(smooth_forecasting, labels_smooth)\n",
    "\n",
    "                self.add_metric(smooth_loss, name=f'smooth_{smooth}_loss', aggregation='mean')\n",
    "                \n",
    "\n",
    "                return smooth_forecasting, smooth_state, smooth_loss\n",
    "\n",
    "            def one_step_forecast_task(self,second_level_state, labels, step=0):\n",
    "                step_state = self.one_step_forecast_lstm_layers[str(step)](second_level_state)\n",
    "                step_forecast = self.one_step_forecast[str(step)](step_state)\n",
    "                step_loss = tf.keras.losses.MeanSquaredError()(step_forecast, labels[:, step, :])\n",
    "\n",
    "                self.add_metric(step_loss, name=f'step_{step}_loss', aggregation='mean')\n",
    "                \n",
    "                \n",
    "                return step_forecast, step_state, step_loss\n",
    "\n",
    "            def day_of_week_forecasting(self, second_level_state, labels_extra):\n",
    "                week_forecast = self.day_of_week_forecasting(second_level_state[:, -1, :])\n",
    "                week_loss = tf.keras.losses.MeanSquaredError()(week_forecast, self.get_mode(labels_extra[:, :, 0], axis=1))\n",
    "\n",
    "                self.add_metric(week_loss, name=f'week_forecasting_loss', aggregation='mean')\n",
    "\n",
    "            def day_of_week_sequence(self, first_level_state, inputs_extra):\n",
    "                # TODO: ONEHOT\n",
    "                week_forecast = self.day_of_week_sequence(first_level_state[:, -1, :])\n",
    "                week_loss = tf.keras.losses.MeanSquaredError()(week_forecast, self.get_mode(inputs_extra[:, :, 0], axis=1))\n",
    "\n",
    "                self.add_metric(week_loss, name=f'week_sequence_loss', aggregation='mean') \n",
    "\n",
    "            def quarter_forecasting(self, second_level_state, labels_extra):\n",
    "                quarter_forecast = self.quarter_forecasting(second_level_state[:, -1, :])\n",
    "                quarter_loss = tf.keras.losses.MeanSquaredError()(quarter_forecast, self.get_mode(labels_extra[:, :, 2], axis=1))\n",
    "\n",
    "                self.add_metric(quarter_loss, name=f'quarter_forecasting_loss', aggregation='mean')\n",
    "\n",
    "            def quarter_sequence(self, first_level_state, inputs_extra):\n",
    "                quarter_forecast = self.quarter_sequence(first_level_state[:, -1, :])\n",
    "                quarter_loss = tf.keras.losses.MeanSquaredError()(quarter_forecast, self.get_mode(inputs_extra[:, :, 2], axis=1))\n",
    "\n",
    "                self.add_metric(quarter_loss, name=f'quarter_sequence_loss', aggregation='mean') \n",
    "\n",
    "\n",
    "            def month_forecasting(self, second_level_state, labels_extra):\n",
    "                month_forecast = self.month_forecasting(second_level_state[:, -1, :])\n",
    "                month_loss = tf.keras.losses.MeanSquaredError()(month_forecast, self.get_mode(labels_extra[:, :, 1], axis=1))\n",
    "\n",
    "                self.add_metric(month_loss, name=f'month_forecasting_loss', aggregation='mean')\n",
    "\n",
    "\n",
    "            def month_sequence(self, first_level_state, inputs_extra):\n",
    "                month_forecast = self.month_sequence(first_level_state[:, -1, :])\n",
    "                month_loss = tf.keras.losses.MeanSquaredError()(month_forecast, self.get_mode(inputs_extra[:, :, 1], axis=1))\n",
    "\n",
    "                self.add_metric(month_loss, name=f'month_sequence_loss', aggregation='mean')                \n",
    "\n",
    "            def swap_task(self, inputs):\n",
    "\n",
    "                x_swap = tf.identity(inputs)\n",
    "\n",
    "                idx = tf.tile([tf.range(0, window)], (tf.shape(x_swap)[0], 1))\n",
    "\n",
    "                swap_index = tf.random.uniform((tf.shape(x_swap)[0], 1), 0, len(self.swap_combinations), dtype=tf.int64)\n",
    "\n",
    "                swap_index_onehot = tf.one_hot(swap_index, len(self.swap_combinations))\n",
    "\n",
    "                swap = tf.gather(tf.convert_to_tensor(self.swap_combinations), swap_index)\n",
    "\n",
    "                swap_numpy = swap.numpy()\n",
    "                idx_numpy = idx.numpy()\n",
    "                idx_numpy = np.concatenate((idx_numpy.reshape(idx_numpy.shape[0], 4, -1)[np.arange(idx_numpy.shape[0]), swap_numpy[..., 0].squeeze()],\n",
    "                            idx_numpy.reshape(idx_numpy.shape[0], 4, -1)[np.arange(idx_numpy.shape[0]), swap_numpy[..., 1].squeeze()],\n",
    "                            idx_numpy.reshape(idx_numpy.shape[0], 4, -1)[np.arange(idx_numpy.shape[0]), swap_numpy[..., 2].squeeze()],\n",
    "                            idx_numpy.reshape(idx_numpy.shape[0], 4, -1)[np.arange(idx_numpy.shape[0]), swap_numpy[..., 3].squeeze()]), axis=1)\n",
    "\n",
    "                idx = tf.convert_to_tensor(idx_numpy)\n",
    "                x_swap = tf.gather(x_swap, idx, axis=1, batch_dims=1)\n",
    "\n",
    "                x_swap_embedding = self.embedding(x_swap)\n",
    "                swap_state = self.first_level_lstm_layer(x_swap_embedding)\n",
    "                swap_state = self.swap_lstm_layer(swap_state)\n",
    "                swap_output = self.swap_output(swap_state)\n",
    "\n",
    "                swap_loss = tf.keras.losses.CategoricalCrossentropy()(tf.squeeze(swap_index_onehot), swap_output)\n",
    "\n",
    "                self.add_metric(swap_loss, name='swap_loss', aggregation='mean')\n",
    "\n",
    "                return swap_output, swap_state\n",
    "\n",
    "            def train_step(self, data):\n",
    "                inputs, labels = data\n",
    "                gradients = []\n",
    "                \n",
    "                with tf.GradientTape(persistent=True) as tape:\n",
    "                    prediction, losses = self(inputs, True)\n",
    "                    loss = self.compiled_loss(labels, prediction, regularization_losses=self.losses)\n",
    "\n",
    "                self.compiled_metrics.update_state(labels, prediction)\n",
    "                \n",
    "                model_layers = np.array(self.layers)\n",
    "\n",
    "                # First level loss\n",
    "                first_level_trainable_weights = [w for layer in model_layers[model.global_first_level_layers] for w in layer.trainable_weights]    \n",
    "                main_grad_first_level = tape.gradient(loss, first_level_trainable_weights)\n",
    "                self.main_grad_first_level_average = update_smooth_grad(main_grad_first_level, self.main_grad_first_level_average, grad_smooth_alpha)\n",
    "\n",
    "                first_level_gradients = []\n",
    "                for l in losses[:2]:\n",
    "\n",
    "                    aux_grad = tape.gradient(l, first_level_trainable_weights)\n",
    "                    aux_grad = combined_grads(main_grad_first_level, self.main_grad_first_level_average, aux_grad, mode, overall, lamb1)\n",
    "\n",
    "                    first_level_gradients.append(aux_grad)\n",
    "\n",
    "                    gradients.extend(list(zip(aux_grad, first_level_trainable_weights)))\n",
    "\n",
    "                first_level_aux_grad = combined_grads(first_level_gradients[0], None, first_level_gradients[1], 'Multitask', overall, 1)\n",
    "                self.first_level_aux_grad_average = update_smooth_grad(first_level_aux_grad, self.first_level_aux_grad_average, grad_smooth_alpha)\n",
    "\n",
    "                #Second level loss\n",
    "                first_level_trainable_weights = [w for layer in model_layers[self.global_first_level_layers] for w in layer.trainable_weights]    \n",
    "\n",
    "                for l in losses[2:]:\n",
    "                    aux_grad = tape.gradient(l, first_level_trainable_weights)\n",
    "                    aux_grad = combined_grads(first_level_aux_grad, self.first_level_aux_grad_average, aux_grad, mode, overall, lamb2)\n",
    "                    gradients.extend(list(zip(aux_grad, first_level_trainable_weights)))\n",
    "\n",
    "                second_level_trainable_weights = [w for layer in model_layers[self.global_second_level_layers] for w in layer.trainable_weights]    \n",
    "                main_grad_second_level = tape.gradient(loss, second_level_trainable_weights)\n",
    "                self.main_grad_second_level_average = update_smooth_grad(main_grad_second_level, self.main_grad_second_level_average, grad_smooth_alpha)\n",
    "\n",
    "                for l in losses[2:]:\n",
    "                    aux_grad = tape.gradient(l, second_level_trainable_weights)\n",
    "                    aux_grad = combined_grads(main_grad_second_level, self.main_grad_second_level_average, aux_grad, mode, overall, lamb2)\n",
    "\n",
    "                    gradients.extend(list(zip(aux_grad, second_level_trainable_weights)))\n",
    "\n",
    "                # All losses\n",
    "                rest_layers = sum([self.global_second_level_layers, self.global_first_level_layers], [])\n",
    "\n",
    "                mask = np.ones(len(model_layers), bool)\n",
    "                mask[rest_layers] = False\n",
    "                all_level_trainable_weights = [w for layer in model_layers[mask] for w in layer.trainable_weights]    \n",
    "                main_grad_rest_layers = tape.gradient(loss, all_level_trainable_weights)\n",
    "\n",
    "                for l in losses:\n",
    "                    grad = tape.gradient(l, all_level_trainable_weights)\n",
    "                    gradients.extend(list(zip(grad, all_level_trainable_weights)))\n",
    "                gradients.extend(list(zip(main_grad_rest_layers, all_level_trainable_weights)))\n",
    "                #Apply all gradients\n",
    "                self.optimizer.apply_gradients(gradients)\n",
    "\n",
    "                return {m.name: m.result() for m in self.metrics}\n",
    "            \n",
    "            def call(self, inputs, training):\n",
    "                labels, labels_extra = inputs[1]\n",
    "                inputs, inputs_extra = inputs[0]\n",
    "\n",
    "                x = self.global_embedding_1(inputs)\n",
    "                x = self.global_embedding_2(x)\n",
    "                #first_level_state = self.first_level_lstm_layer(x)\n",
    "\n",
    "                # First level\n",
    "                gap_filling_outputs, gap_loss = self.gap_filling_task(inputs)\n",
    "                noise_reduction_outputs, noise_loss = self.noise_reduction_task(inputs)\n",
    "                #swap_output, swap_state = self.swap_task(inputs)\n",
    "\n",
    "                #Second level\n",
    "                second_level_state = self.second_level_lstm_layer(x)\n",
    "                \"\"\"smooth_forecasting_8, smooth_state_8, smooth_loss_8 = self.smooth_forecast_task(second_level_state, labels, 8)\n",
    "                smooth_forecasting_6, smooth_state_6, smooth_loss_6 = self.smooth_forecast_task(second_level_state, labels, 6)\n",
    "                smooth_forecasting_3, smooth_state_3, smooth_loss_3 = self.smooth_forecast_task(second_level_state, labels, 3)\n",
    "                \"\"\"\n",
    "                first_step_forecast, first_step_state, first_step_loss = self.one_step_forecast_task(second_level_state, labels, 0)\n",
    "                mid_step_forecast, mid_step_state, mid_step_loss = self.one_step_forecast_task(second_level_state, labels, self.horizon//2)\n",
    "                last_step_forecast, last_step_state, last_step_loss = self.one_step_forecast_task(second_level_state, labels, self.horizon-1)\n",
    "                \n",
    "                losses = [gap_loss, noise_loss, first_step_loss, mid_step_loss, last_step_loss]\n",
    "                # smooth_loss_8, smooth_loss_6, smooth_loss_3\n",
    "                \n",
    "                \"\"\"tasks_loss = 0\n",
    "                for i, loss in enumerate(losses):\n",
    "                    losses[i] = (0.5/self.loss_weigts[i]**2)*loss + tf.math.log(1+self.loss_weigts[i]**2)\"\"\"\n",
    "                    \n",
    "                #self.add_loss(tasks_loss)\n",
    "                # Last level\n",
    "                prediction_level_state = self.prediction_level_lstm_layer(second_level_state)\n",
    "                prediction_state = tf.keras.layers.concatenate([prediction_level_state, last_step_state, mid_step_state, \n",
    "                                         first_step_state])\n",
    "                \n",
    "                prediction_output = self.prediction_output(prediction_state)\n",
    "                \n",
    "                if training:\n",
    "                    return prediction_output, losses\n",
    "                else:\n",
    "                    return prediction_output\n",
    "\n",
    "\n",
    "        model = MultitaskHLNet(d_model, n_features, window, horizon, batch_size, grad_smooth_alpha, lamb1, lamb2, mode, overall)\n",
    "\n",
    "        loss = 'mse'\n",
    "    elif model_type == 'multitaskhlnet_taskslstm_firstlevelcnn_gradsprojected_embeddingappended':\n",
    "    #pred_index = -1\n",
    "\n",
    "        class MultitaskHLNet(tf.keras.Model):\n",
    "\n",
    "            def __init__(self, d_model, n_features, window, horizon, batch_size, grad_smooth_alpha, lamb1, lamb2, mode, overall):\n",
    "                super(MultitaskHLNet, self).__init__()\n",
    "                self.d_model = d_model\n",
    "                self.n_features = n_features\n",
    "                self.window = window\n",
    "                self.horizon = horizon\n",
    "                self.batch_size = batch_size\n",
    "                self.main_grad_first_level_average = None\n",
    "                self.main_grad_second_level_average = None\n",
    "                self.first_level_aux_grad_average = None\n",
    "                self.grad_smooth_alpha = grad_smooth_alpha\n",
    "                self.lamb1 = lamb1\n",
    "                self.lamb2 = lamb2\n",
    "                self.mode = mode\n",
    "                self.overall = overall\n",
    "                \n",
    "                self.loss_weigts = tf.Variable(tf.ones([8]))\n",
    "                \n",
    "                self.global_embedding_1 = tf.keras.layers.Conv1D(d_model//2, 3, activation='relu', input_shape=(window, 3), padding='causal')\n",
    "                self.global_embedding_2 = tf.keras.layers.Conv1D(d_model, 3, activation='relu', padding='causal')\n",
    "                \n",
    "                self.global_first_level_layers = [0,1]\n",
    "                \"\"\"\n",
    "                    First Level tasks - Sequence featurization\n",
    "                \"\"\"\n",
    "                self.decode_cnn_1 = tf.keras.layers.Conv1DTranspose(d_model, 3, activation='relu', padding='same')\n",
    "                self.decode_cnn_2 = tf.keras.layers.Conv1DTranspose(d_model//2, 3, activation='relu', padding='same')\n",
    "                self.gap_filling_output = tf.keras.layers.Dense(n_features)\n",
    "\n",
    "                self.gaussian_noise = tf.keras.layers.GaussianNoise(0.01)\n",
    "                self.noise_reduction_output = tf.keras.layers.Dense(n_features)\n",
    "\n",
    "                #self.swap_combinations = list(itertools.permutations(np.arange(0, 4), 4))\n",
    "                #self.swap_lstm_layer = tf.keras.layers.LSTM(d_model, name='swap_lstm', activation='relu')\n",
    "                #self.swap_output = tf.keras.layers.Dense(len(self.swap_combinations), name='swap_task', activation='softmax')\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                    Second Level tasks - Forecasting helpers\n",
    "                \"\"\"\n",
    "                self.second_level_lstm_layer = tf.keras.layers.LSTM(d_model, return_sequences=True)\n",
    "\n",
    "                self.smooth_lstm_layers = {'8': tf.keras.layers.LSTM(d_model, name='smooth_forecasting_8_lstm', activation='relu'),\n",
    "                                '6':  tf.keras.layers.LSTM(d_model, name='smooth_forecasting_6_lstm', activation='relu'),\n",
    "                                '3':  tf.keras.layers.LSTM(d_model, name='smooth_forecasting_3_lstm', activation='relu')}\n",
    "\n",
    "                self.smooth_layers = {'8': tf.keras.layers.Dense(horizon-8+1, name='smooth_forecasting_8'),\n",
    "                                '6': tf.keras.layers.Dense(horizon-6+1, name='smooth_forecasting_6'),\n",
    "                                '3': tf.keras.layers.Dense(horizon-3+1, name='smooth_forecasting_3')}\n",
    "\n",
    "\n",
    "                self.one_step_forecast_lstm_layers = {'0': tf.keras.layers.LSTM(d_model, name='next_step_forecasting_lstm', activation='relu'),\n",
    "                                str(horizon//2):  tf.keras.layers.LSTM(d_model, name='mid_step_forecasting_lstm', activation='relu'),\n",
    "                                str(horizon-1):  tf.keras.layers.LSTM(d_model, name='last_step_forecasting_lstm', activation='relu')}\n",
    "\n",
    "                self.one_step_forecast = {'0': tf.keras.layers.Dense(1, name='next_step_forecasting'),\n",
    "                                    str(horizon//2): tf.keras.layers.Dense(1, name='mid_step_forecasting'),\n",
    "                                    str(horizon-1): tf.keras.layers.Dense(1, name='last_step_forecasting')}\n",
    "                self.global_second_level_layers = [7]\n",
    "                \"\"\"\n",
    "                    Last Level tasks\n",
    "                \"\"\"\n",
    "                self.prediction_level_lstm_layer = tf.keras.layers.LSTM(d_model, return_sequences=False)\n",
    "                self.prediction_output = tf.keras.layers.Dense(horizon, name='prediction')\n",
    "\n",
    "            def get_mode(self, x, axis=1):\n",
    "                dt = x.dtype\n",
    "                # Shift input in case it has negative values\n",
    "                m = tf.math.reduce_min(x)\n",
    "                x2 = x - m\n",
    "                # minlength should not be necessary but may fail without it\n",
    "                # (reported here https://github.com/tensorflow/probability/issues/962)\n",
    "                c = tfp.stats.count_integers(x2, axis=axis, dtype=dt,\n",
    "                                             minlength=tf.math.reduce_max(x2) + 1)\n",
    "                # Find the values with largest counts\n",
    "                idx = tf.math.argmax(c, axis=0, output_type=dt)\n",
    "                # Get the modes by shifting by the subtracted minimum\n",
    "                modes = idx + m\n",
    "                # Get the number of counts\n",
    "                counts = tf.math.reduce_max(c, axis=0)\n",
    "\n",
    "                return modes\n",
    "\n",
    "            def gap_filling_task(self, inputs):\n",
    "\n",
    "                #Gap filling task\n",
    "                batch_indexes = tf.tile(tf.range(tf.shape(inputs)[0])[:, tf.newaxis, tf.newaxis], (1, self.window, 1))\n",
    "                head_indexes = tf.tile(tf.range(self.window)[tf.newaxis, :, tf.newaxis], (tf.shape(inputs)[0], 1, 1))\n",
    "                feat_index = tf.random.uniform((tf.shape(inputs)[0],window,1), minval=0, maxval=3, dtype=tf.int32)\n",
    "\n",
    "                idx = tf.squeeze(tf.stack(values=[batch_indexes, head_indexes, feat_index], axis=-1))\n",
    "                idx = tf.transpose(idx, perm=(1,2,0))    \n",
    "\n",
    "                gap_index =  tf.reshape(tf.transpose(tf.random.shuffle(idx), perm= (2, 0, 1))[:, :1, :], (-1, 3))\n",
    "\n",
    "                x_gap = tf.identity(inputs)\n",
    "                x_gap_updated = tf.tensor_scatter_nd_update(x_gap, indices = gap_index, updates = -tf.ones(gap_index.shape[0])*100)\n",
    "                x_gap_embedding = self.global_embedding_1(x_gap_updated)\n",
    "                x_gap_embedding = self.global_embedding_2(x_gap_embedding)\n",
    "                x_gap_embedding = self.decode_cnn_1(x_gap_embedding)\n",
    "                x_gap_embedding = self.decode_cnn_2(x_gap_embedding)\n",
    "                #gap_state = self.first_level_lstm_layer(x_gap_embedding)\n",
    "                gap_filling_output = self.gap_filling_output(x_gap_embedding)\n",
    "                #gap_filling_output = self.gap_filling_reshape(gap_filling_output_flat)\n",
    "                #gap_filling_true = tf.reshape(tf.gather_nd(x_gap, gap_index), (tf.shape(x_gap)[0], 1))\n",
    "                gap_loss = tf.keras.losses.MeanSquaredError()(gap_filling_output, x_gap)\n",
    "\n",
    "                self.add_metric(gap_loss, name='gap_loss', aggregation='mean')\n",
    "                \n",
    "\n",
    "                return gap_filling_output, gap_loss, x_gap_embedding\n",
    "\n",
    "            def noise_reduction_task(self, inputs):\n",
    "\n",
    "                input_copy = tf.identity(inputs)\n",
    "                x_noise = self.gaussian_noise(input_copy)\n",
    "\n",
    "                x_noise_embedding = self.global_embedding_1(x_noise)\n",
    "                x_noise_embedding = self.global_embedding_2(x_noise_embedding)\n",
    "                x_noise_embedding = self.decode_cnn_1(x_noise_embedding)\n",
    "                x_noise_embedding = self.decode_cnn_2(x_noise_embedding)\n",
    "                #noise_state = self.first_level_lstm_layer(x_noise_embedding)\n",
    "                #noise_state = self.noise_reduction_lstm_layer(noise_state)\n",
    "                noise_output = self.noise_reduction_output(x_noise_embedding)\n",
    "                #noise_output = self.noise_reduction_reshape(noise_output_flat)\n",
    "\n",
    "                noise_loss = tf.keras.losses.MeanSquaredError()(noise_output, input_copy)\n",
    "\n",
    "                self.add_metric(noise_loss, name='noise_loss', aggregation='mean')\n",
    "                \n",
    "\n",
    "                return noise_output, noise_loss, x_noise_embedding\n",
    "\n",
    "            def smooth_forecast_task(self, second_level_state, labels, smooth=6):\n",
    "\n",
    "                smooth_state = self.smooth_lstm_layers[str(smooth)](second_level_state)\n",
    "                smooth_forecasting = self.smooth_layers[str(smooth)](smooth_state)\n",
    "                labels_smooth = tf.math.reduce_mean(tf.signal.frame(labels, smooth, 1, axis=1), axis=2)\n",
    "\n",
    "                smooth_loss = tf.keras.losses.MeanSquaredError()(smooth_forecasting, labels_smooth)\n",
    "\n",
    "                self.add_metric(smooth_loss, name=f'smooth_{smooth}_loss', aggregation='mean')\n",
    "                \n",
    "\n",
    "                return smooth_forecasting, smooth_state, smooth_loss\n",
    "\n",
    "            def one_step_forecast_task(self,second_level_state, labels, step=0):\n",
    "                step_state = self.one_step_forecast_lstm_layers[str(step)](second_level_state)\n",
    "                step_forecast = self.one_step_forecast[str(step)](step_state)\n",
    "                step_loss = tf.keras.losses.MeanSquaredError()(step_forecast, labels[:, step, :])\n",
    "\n",
    "                self.add_metric(step_loss, name=f'step_{step}_loss', aggregation='mean')\n",
    "                \n",
    "                \n",
    "                return step_forecast, step_state, step_loss\n",
    "\n",
    "            def day_of_week_forecasting(self, second_level_state, labels_extra):\n",
    "                week_forecast = self.day_of_week_forecasting(second_level_state[:, -1, :])\n",
    "                week_loss = tf.keras.losses.MeanSquaredError()(week_forecast, self.get_mode(labels_extra[:, :, 0], axis=1))\n",
    "\n",
    "                self.add_metric(week_loss, name=f'week_forecasting_loss', aggregation='mean')\n",
    "\n",
    "            def day_of_week_sequence(self, first_level_state, inputs_extra):\n",
    "                # TODO: ONEHOT\n",
    "                week_forecast = self.day_of_week_sequence(first_level_state[:, -1, :])\n",
    "                week_loss = tf.keras.losses.MeanSquaredError()(week_forecast, self.get_mode(inputs_extra[:, :, 0], axis=1))\n",
    "\n",
    "                self.add_metric(week_loss, name=f'week_sequence_loss', aggregation='mean') \n",
    "\n",
    "            def quarter_forecasting(self, second_level_state, labels_extra):\n",
    "                quarter_forecast = self.quarter_forecasting(second_level_state[:, -1, :])\n",
    "                quarter_loss = tf.keras.losses.MeanSquaredError()(quarter_forecast, self.get_mode(labels_extra[:, :, 2], axis=1))\n",
    "\n",
    "                self.add_metric(quarter_loss, name=f'quarter_forecasting_loss', aggregation='mean')\n",
    "\n",
    "            def quarter_sequence(self, first_level_state, inputs_extra):\n",
    "                quarter_forecast = self.quarter_sequence(first_level_state[:, -1, :])\n",
    "                quarter_loss = tf.keras.losses.MeanSquaredError()(quarter_forecast, self.get_mode(inputs_extra[:, :, 2], axis=1))\n",
    "\n",
    "                self.add_metric(quarter_loss, name=f'quarter_sequence_loss', aggregation='mean') \n",
    "\n",
    "\n",
    "            def month_forecasting(self, second_level_state, labels_extra):\n",
    "                month_forecast = self.month_forecasting(second_level_state[:, -1, :])\n",
    "                month_loss = tf.keras.losses.MeanSquaredError()(month_forecast, self.get_mode(labels_extra[:, :, 1], axis=1))\n",
    "\n",
    "                self.add_metric(month_loss, name=f'month_forecasting_loss', aggregation='mean')\n",
    "\n",
    "\n",
    "            def month_sequence(self, first_level_state, inputs_extra):\n",
    "                month_forecast = self.month_sequence(first_level_state[:, -1, :])\n",
    "                month_loss = tf.keras.losses.MeanSquaredError()(month_forecast, self.get_mode(inputs_extra[:, :, 1], axis=1))\n",
    "\n",
    "                self.add_metric(month_loss, name=f'month_sequence_loss', aggregation='mean')                \n",
    "\n",
    "            def swap_task(self, inputs):\n",
    "\n",
    "                x_swap = tf.identity(inputs)\n",
    "\n",
    "                idx = tf.tile([tf.range(0, window)], (tf.shape(x_swap)[0], 1))\n",
    "\n",
    "                swap_index = tf.random.uniform((tf.shape(x_swap)[0], 1), 0, len(self.swap_combinations), dtype=tf.int64)\n",
    "\n",
    "                swap_index_onehot = tf.one_hot(swap_index, len(self.swap_combinations))\n",
    "\n",
    "                swap = tf.gather(tf.convert_to_tensor(self.swap_combinations), swap_index)\n",
    "\n",
    "                swap_numpy = swap.numpy()\n",
    "                idx_numpy = idx.numpy()\n",
    "                idx_numpy = np.concatenate((idx_numpy.reshape(idx_numpy.shape[0], 4, -1)[np.arange(idx_numpy.shape[0]), swap_numpy[..., 0].squeeze()],\n",
    "                            idx_numpy.reshape(idx_numpy.shape[0], 4, -1)[np.arange(idx_numpy.shape[0]), swap_numpy[..., 1].squeeze()],\n",
    "                            idx_numpy.reshape(idx_numpy.shape[0], 4, -1)[np.arange(idx_numpy.shape[0]), swap_numpy[..., 2].squeeze()],\n",
    "                            idx_numpy.reshape(idx_numpy.shape[0], 4, -1)[np.arange(idx_numpy.shape[0]), swap_numpy[..., 3].squeeze()]), axis=1)\n",
    "\n",
    "                idx = tf.convert_to_tensor(idx_numpy)\n",
    "                x_swap = tf.gather(x_swap, idx, axis=1, batch_dims=1)\n",
    "\n",
    "                x_swap_embedding = self.embedding(x_swap)\n",
    "                swap_state = self.first_level_lstm_layer(x_swap_embedding)\n",
    "                swap_state = self.swap_lstm_layer(swap_state)\n",
    "                swap_output = self.swap_output(swap_state)\n",
    "\n",
    "                swap_loss = tf.keras.losses.CategoricalCrossentropy()(tf.squeeze(swap_index_onehot), swap_output)\n",
    "\n",
    "                self.add_metric(swap_loss, name='swap_loss', aggregation='mean')\n",
    "\n",
    "                return swap_output, swap_state\n",
    "\n",
    "            def train_step(self, data):\n",
    "                inputs, labels = data\n",
    "                gradients = []\n",
    "                \n",
    "                with tf.GradientTape(persistent=True) as tape:\n",
    "                    prediction, losses = self(inputs, True)\n",
    "                    loss = self.compiled_loss(labels, prediction, regularization_losses=self.losses)\n",
    "\n",
    "                self.compiled_metrics.update_state(labels, prediction)\n",
    "                \n",
    "                model_layers = np.array(self.layers)\n",
    "\n",
    "                # First level loss\n",
    "                first_level_trainable_weights = [w for layer in model_layers[model.global_first_level_layers] for w in layer.trainable_weights]    \n",
    "                main_grad_first_level = tape.gradient(loss, first_level_trainable_weights)\n",
    "                self.main_grad_first_level_average = update_smooth_grad(main_grad_first_level, self.main_grad_first_level_average, grad_smooth_alpha)\n",
    "\n",
    "                first_level_gradients = []\n",
    "                for l in losses[:2]:\n",
    "\n",
    "                    aux_grad = tape.gradient(l, first_level_trainable_weights)\n",
    "                    aux_grad = combined_grads(main_grad_first_level, self.main_grad_first_level_average, aux_grad, mode, overall, lamb1)\n",
    "\n",
    "                    first_level_gradients.append(aux_grad)\n",
    "\n",
    "                    gradients.extend(list(zip(aux_grad, first_level_trainable_weights)))\n",
    "\n",
    "                first_level_aux_grad = combined_grads(first_level_gradients[0], None, first_level_gradients[1], 'Multitask', overall, 1)\n",
    "                self.first_level_aux_grad_average = update_smooth_grad(first_level_aux_grad, self.first_level_aux_grad_average, grad_smooth_alpha)\n",
    "\n",
    "                #Second level loss\n",
    "                first_level_trainable_weights = [w for layer in model_layers[self.global_first_level_layers] for w in layer.trainable_weights]    \n",
    "\n",
    "                for l in losses[2:]:\n",
    "                    aux_grad = tape.gradient(l, first_level_trainable_weights)\n",
    "                    aux_grad = combined_grads(first_level_aux_grad, self.first_level_aux_grad_average, aux_grad, mode, overall, lamb2)\n",
    "                    gradients.extend(list(zip(aux_grad, first_level_trainable_weights)))\n",
    "\n",
    "                second_level_trainable_weights = [w for layer in model_layers[self.global_second_level_layers] for w in layer.trainable_weights]    \n",
    "                main_grad_second_level = tape.gradient(loss, second_level_trainable_weights)\n",
    "                self.main_grad_second_level_average = update_smooth_grad(main_grad_second_level, self.main_grad_second_level_average, grad_smooth_alpha)\n",
    "\n",
    "                for l in losses[2:]:\n",
    "                    aux_grad = tape.gradient(l, second_level_trainable_weights)\n",
    "                    aux_grad = combined_grads(main_grad_second_level, self.main_grad_second_level_average, aux_grad, mode, overall, lamb2)\n",
    "\n",
    "                    gradients.extend(list(zip(aux_grad, second_level_trainable_weights)))\n",
    "\n",
    "                # All losses\n",
    "                rest_layers = sum([self.global_second_level_layers, self.global_first_level_layers], [])\n",
    "\n",
    "                mask = np.ones(len(model_layers), bool)\n",
    "                mask[rest_layers] = False\n",
    "                all_level_trainable_weights = [w for layer in model_layers[mask] for w in layer.trainable_weights]    \n",
    "                main_grad_rest_layers = tape.gradient(loss, all_level_trainable_weights)\n",
    "\n",
    "                for l in losses:\n",
    "                    grad = tape.gradient(l, all_level_trainable_weights)\n",
    "                    gradients.extend(list(zip(grad, all_level_trainable_weights)))\n",
    "                    \n",
    "                gradients.extend(list(zip(main_grad_rest_layers, all_level_trainable_weights)))\n",
    "                #Apply all gradients\n",
    "                self.optimizer.apply_gradients(gradients)\n",
    "\n",
    "                return {m.name: m.result() for m in self.metrics}\n",
    "            \n",
    "            def call(self, inputs, training):\n",
    "                labels, labels_extra = inputs[1]\n",
    "                inputs, inputs_extra = inputs[0]\n",
    "\n",
    "                x = self.global_embedding_1(inputs)\n",
    "                x = self.global_embedding_2(x)\n",
    "                #first_level_state = self.first_level_lstm_layer(x)\n",
    "\n",
    "                # First level\n",
    "                gap_filling_outputs, gap_loss, x_gap_embedding = self.gap_filling_task(inputs)\n",
    "                noise_reduction_outputs, noise_loss, x_noise_embedding = self.noise_reduction_task(inputs)\n",
    "                #swap_output, swap_state = self.swap_task(inputs)\n",
    "                \n",
    "                x = tf.keras.layers.concatenate([x, x_gap_embedding, x_noise_embedding])\n",
    "                \n",
    "                #Second level\n",
    "                second_level_state = self.second_level_lstm_layer(x)\n",
    "                smooth_forecasting_8, smooth_state_8, smooth_loss_8 = self.smooth_forecast_task(second_level_state, labels, 8)\n",
    "                smooth_forecasting_6, smooth_state_6, smooth_loss_6 = self.smooth_forecast_task(second_level_state, labels, 6)\n",
    "                smooth_forecasting_3, smooth_state_3, smooth_loss_3 = self.smooth_forecast_task(second_level_state, labels, 3)\n",
    "                \n",
    "                first_step_forecast, first_step_state, first_step_loss = self.one_step_forecast_task(second_level_state, labels, 0)\n",
    "                mid_step_forecast, mid_step_state, mid_step_loss = self.one_step_forecast_task(second_level_state, labels, self.horizon//2)\n",
    "                last_step_forecast, last_step_state, last_step_loss = self.one_step_forecast_task(second_level_state, labels, self.horizon-1)\n",
    "                \n",
    "                losses = [gap_loss, noise_loss, first_step_loss, mid_step_loss, last_step_loss, smooth_loss_8, smooth_loss_6, smooth_loss_3]\n",
    "                # \n",
    "                \n",
    "                \"\"\"tasks_loss = 0\n",
    "                for i, loss in enumerate(losses):\n",
    "                    losses[i] = (0.5/self.loss_weigts[i]**2)*loss + tf.math.log(1+self.loss_weigts[i]**2)\"\"\"\n",
    "                    \n",
    "                #self.add_loss(tasks_loss)\n",
    "                # Last level\n",
    "                prediction_level_state = self.prediction_level_lstm_layer(second_level_state)\n",
    "                prediction_state = tf.keras.layers.concatenate([prediction_level_state, last_step_state, mid_step_state, \n",
    "                                         first_step_state, smooth_state_3, smooth_state_6, smooth_state_8])\n",
    "                \n",
    "                prediction_output = self.prediction_output(prediction_state)\n",
    "                \n",
    "                if training:\n",
    "                    return prediction_output, losses\n",
    "                else:\n",
    "                    return prediction_output\n",
    "\n",
    "\n",
    "        model = MultitaskHLNet(d_model, n_features, window, horizon, batch_size, grad_smooth_alpha, lamb1, lamb2, mode, overall)\n",
    "\n",
    "        loss = 'mse'\n",
    "    elif model_type == 'multitaskhlnet_taskslstm_firstlevelcnn_gradsprojected_embeddingappended_isolated':\n",
    "    #pred_index = -1\n",
    "\n",
    "        class MultitaskHLNet(tf.keras.Model):\n",
    "\n",
    "            def __init__(self, d_model, n_features, window, horizon, batch_size, grad_smooth_alpha, lamb1, lamb2, mode, overall):\n",
    "                super(MultitaskHLNet, self).__init__()\n",
    "                self.d_model = d_model\n",
    "                self.n_features = n_features\n",
    "                self.window = window\n",
    "                self.horizon = horizon\n",
    "                self.batch_size = batch_size\n",
    "                self.main_grad_first_level_average = None\n",
    "                self.main_grad_second_level_average = None\n",
    "                self.first_level_aux_grad_average = None\n",
    "                self.grad_smooth_alpha = grad_smooth_alpha\n",
    "                self.lamb1 = lamb1\n",
    "                self.lamb2 = lamb2\n",
    "                self.mode = mode\n",
    "                self.overall = overall\n",
    "                \n",
    "                self.loss_weigts = tf.Variable(tf.ones([8]))\n",
    "                \n",
    "                self.global_embedding_1 = tf.keras.layers.Conv1D(d_model//2, 3, activation='relu', input_shape=(window, 3), padding='causal')\n",
    "                self.global_embedding_2 = tf.keras.layers.Conv1D(d_model, 3, activation='relu', padding='causal')\n",
    "                \n",
    "                self.global_first_level_layers = [0,1]\n",
    "                \"\"\"\n",
    "                    First Level tasks - Sequence featurization\n",
    "                \"\"\"\n",
    "                self.decode_cnn_1 = tf.keras.layers.Conv1DTranspose(d_model, 3, activation='relu', padding='same')\n",
    "                self.decode_cnn_2 = tf.keras.layers.Conv1DTranspose(d_model//2, 3, activation='relu', padding='same')\n",
    "                self.gap_filling_output = tf.keras.layers.Dense(n_features)\n",
    "\n",
    "                self.gaussian_noise = tf.keras.layers.GaussianNoise(0.01)\n",
    "                self.noise_reduction_output = tf.keras.layers.Dense(n_features)\n",
    "\n",
    "                #self.swap_combinations = list(itertools.permutations(np.arange(0, 4), 4))\n",
    "                #self.swap_lstm_layer = tf.keras.layers.LSTM(d_model, name='swap_lstm', activation='relu')\n",
    "                #self.swap_output = tf.keras.layers.Dense(len(self.swap_combinations), name='swap_task', activation='softmax')\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                    Second Level tasks - Forecasting helpers\n",
    "                \"\"\"\n",
    "                self.second_level_lstm_layer = tf.keras.layers.LSTM(d_model, return_sequences=True)\n",
    "\n",
    "                self.smooth_lstm_layers = {'8': tf.keras.layers.LSTM(d_model, name='smooth_forecasting_8_lstm', activation='relu'),\n",
    "                                '6':  tf.keras.layers.LSTM(d_model, name='smooth_forecasting_6_lstm', activation='relu'),\n",
    "                                '3':  tf.keras.layers.LSTM(d_model, name='smooth_forecasting_3_lstm', activation='relu')}\n",
    "\n",
    "                self.smooth_layers = {'8': tf.keras.layers.Dense(horizon-8+1, name='smooth_forecasting_8'),\n",
    "                                '6': tf.keras.layers.Dense(horizon-6+1, name='smooth_forecasting_6'),\n",
    "                                '3': tf.keras.layers.Dense(horizon-3+1, name='smooth_forecasting_3')}\n",
    "\n",
    "\n",
    "                self.one_step_forecast_lstm_layers = {'0': tf.keras.layers.LSTM(d_model, name='next_step_forecasting_lstm', activation='relu'),\n",
    "                                str(horizon//2):  tf.keras.layers.LSTM(d_model, name='mid_step_forecasting_lstm', activation='relu'),\n",
    "                                str(horizon-1):  tf.keras.layers.LSTM(d_model, name='last_step_forecasting_lstm', activation='relu')}\n",
    "\n",
    "                self.one_step_forecast = {'0': tf.keras.layers.Dense(1, name='next_step_forecasting'),\n",
    "                                    str(horizon//2): tf.keras.layers.Dense(1, name='mid_step_forecasting'),\n",
    "                                    str(horizon-1): tf.keras.layers.Dense(1, name='last_step_forecasting')}\n",
    "                self.global_second_level_layers = [7]\n",
    "                \"\"\"\n",
    "                    Last Level tasks\n",
    "                \"\"\"\n",
    "                self.prediction_level_lstm_layer = tf.keras.layers.LSTM(d_model, return_sequences=False)\n",
    "                self.prediction_output = tf.keras.layers.Dense(horizon, name='prediction')\n",
    "\n",
    "            def get_mode(self, x, axis=1):\n",
    "                dt = x.dtype\n",
    "                # Shift input in case it has negative values\n",
    "                m = tf.math.reduce_min(x)\n",
    "                x2 = x - m\n",
    "                # minlength should not be necessary but may fail without it\n",
    "                # (reported here https://github.com/tensorflow/probability/issues/962)\n",
    "                c = tfp.stats.count_integers(x2, axis=axis, dtype=dt,\n",
    "                                             minlength=tf.math.reduce_max(x2) + 1)\n",
    "                # Find the values with largest counts\n",
    "                idx = tf.math.argmax(c, axis=0, output_type=dt)\n",
    "                # Get the modes by shifting by the subtracted minimum\n",
    "                modes = idx + m\n",
    "                # Get the number of counts\n",
    "                counts = tf.math.reduce_max(c, axis=0)\n",
    "\n",
    "                return modes\n",
    "\n",
    "            def gap_filling_task(self, inputs):\n",
    "\n",
    "                #Gap filling task\n",
    "                batch_indexes = tf.tile(tf.range(tf.shape(inputs)[0])[:, tf.newaxis, tf.newaxis], (1, self.window, 1))\n",
    "                head_indexes = tf.tile(tf.range(self.window)[tf.newaxis, :, tf.newaxis], (tf.shape(inputs)[0], 1, 1))\n",
    "                feat_index = tf.random.uniform((tf.shape(inputs)[0],window,1), minval=0, maxval=3, dtype=tf.int32)\n",
    "\n",
    "                idx = tf.squeeze(tf.stack(values=[batch_indexes, head_indexes, feat_index], axis=-1))\n",
    "                idx = tf.transpose(idx, perm=(1,2,0))    \n",
    "\n",
    "                gap_index =  tf.reshape(tf.transpose(tf.random.shuffle(idx), perm= (2, 0, 1))[:, :1, :], (-1, 3))\n",
    "\n",
    "                x_gap = tf.identity(inputs)\n",
    "                x_gap_updated = tf.tensor_scatter_nd_update(x_gap, indices = gap_index, updates = -tf.ones(gap_index.shape[0])*100)\n",
    "                x_gap_embedding = self.global_embedding_1(x_gap_updated)\n",
    "                x_gap_embedding = self.global_embedding_2(x_gap_embedding)\n",
    "                x_gap_embedding = self.decode_cnn_1(x_gap_embedding)\n",
    "                x_gap_embedding = self.decode_cnn_2(x_gap_embedding)\n",
    "                #gap_state = self.first_level_lstm_layer(x_gap_embedding)\n",
    "                gap_filling_output = self.gap_filling_output(x_gap_embedding)\n",
    "                #gap_filling_output = self.gap_filling_reshape(gap_filling_output_flat)\n",
    "                #gap_filling_true = tf.reshape(tf.gather_nd(x_gap, gap_index), (tf.shape(x_gap)[0], 1))\n",
    "                gap_loss = tf.keras.losses.MeanSquaredError()(gap_filling_output, x_gap)\n",
    "\n",
    "                self.add_metric(gap_loss, name='gap_loss', aggregation='mean')\n",
    "                \n",
    "\n",
    "                return gap_filling_output, gap_loss, x_gap_embedding\n",
    "\n",
    "            def noise_reduction_task(self, inputs):\n",
    "\n",
    "                input_copy = tf.identity(inputs)\n",
    "                x_noise = self.gaussian_noise(input_copy)\n",
    "\n",
    "                x_noise_embedding = self.global_embedding_1(x_noise)\n",
    "                x_noise_embedding = self.global_embedding_2(x_noise_embedding)\n",
    "                x_noise_embedding = self.decode_cnn_1(x_noise_embedding)\n",
    "                x_noise_embedding = self.decode_cnn_2(x_noise_embedding)\n",
    "                #noise_state = self.first_level_lstm_layer(x_noise_embedding)\n",
    "                #noise_state = self.noise_reduction_lstm_layer(noise_state)\n",
    "                noise_output = self.noise_reduction_output(x_noise_embedding)\n",
    "                #noise_output = self.noise_reduction_reshape(noise_output_flat)\n",
    "\n",
    "                noise_loss = tf.keras.losses.MeanSquaredError()(noise_output, input_copy)\n",
    "\n",
    "                self.add_metric(noise_loss, name='noise_loss', aggregation='mean')\n",
    "                \n",
    "\n",
    "                return noise_output, noise_loss, x_noise_embedding\n",
    "\n",
    "            def smooth_forecast_task(self, second_level_state, labels, smooth=6):\n",
    "\n",
    "                smooth_state = self.smooth_lstm_layers[str(smooth)](second_level_state)\n",
    "                smooth_forecasting = self.smooth_layers[str(smooth)](smooth_state)\n",
    "                labels_smooth = tf.math.reduce_mean(tf.signal.frame(labels, smooth, 1, axis=1), axis=2)\n",
    "\n",
    "                smooth_loss = tf.keras.losses.MeanSquaredError()(smooth_forecasting, labels_smooth)\n",
    "\n",
    "                self.add_metric(smooth_loss, name=f'smooth_{smooth}_loss', aggregation='mean')\n",
    "                \n",
    "\n",
    "                return smooth_forecasting, smooth_state, smooth_loss\n",
    "\n",
    "            def one_step_forecast_task(self,second_level_state, labels, step=0):\n",
    "                step_state = self.one_step_forecast_lstm_layers[str(step)](second_level_state)\n",
    "                step_forecast = self.one_step_forecast[str(step)](step_state)\n",
    "                step_loss = tf.keras.losses.MeanSquaredError()(step_forecast, labels[:, step, :])\n",
    "\n",
    "                self.add_metric(step_loss, name=f'step_{step}_loss', aggregation='mean')\n",
    "                \n",
    "                \n",
    "                return step_forecast, step_state, step_loss\n",
    "\n",
    "            def day_of_week_forecasting(self, second_level_state, labels_extra):\n",
    "                week_forecast = self.day_of_week_forecasting(second_level_state[:, -1, :])\n",
    "                week_loss = tf.keras.losses.MeanSquaredError()(week_forecast, self.get_mode(labels_extra[:, :, 0], axis=1))\n",
    "\n",
    "                self.add_metric(week_loss, name=f'week_forecasting_loss', aggregation='mean')\n",
    "\n",
    "            def day_of_week_sequence(self, first_level_state, inputs_extra):\n",
    "                # TODO: ONEHOT\n",
    "                week_forecast = self.day_of_week_sequence(first_level_state[:, -1, :])\n",
    "                week_loss = tf.keras.losses.MeanSquaredError()(week_forecast, self.get_mode(inputs_extra[:, :, 0], axis=1))\n",
    "\n",
    "                self.add_metric(week_loss, name=f'week_sequence_loss', aggregation='mean') \n",
    "\n",
    "            def quarter_forecasting(self, second_level_state, labels_extra):\n",
    "                quarter_forecast = self.quarter_forecasting(second_level_state[:, -1, :])\n",
    "                quarter_loss = tf.keras.losses.MeanSquaredError()(quarter_forecast, self.get_mode(labels_extra[:, :, 2], axis=1))\n",
    "\n",
    "                self.add_metric(quarter_loss, name=f'quarter_forecasting_loss', aggregation='mean')\n",
    "\n",
    "            def quarter_sequence(self, first_level_state, inputs_extra):\n",
    "                quarter_forecast = self.quarter_sequence(first_level_state[:, -1, :])\n",
    "                quarter_loss = tf.keras.losses.MeanSquaredError()(quarter_forecast, self.get_mode(inputs_extra[:, :, 2], axis=1))\n",
    "\n",
    "                self.add_metric(quarter_loss, name=f'quarter_sequence_loss', aggregation='mean') \n",
    "\n",
    "\n",
    "            def month_forecasting(self, second_level_state, labels_extra):\n",
    "                month_forecast = self.month_forecasting(second_level_state[:, -1, :])\n",
    "                month_loss = tf.keras.losses.MeanSquaredError()(month_forecast, self.get_mode(labels_extra[:, :, 1], axis=1))\n",
    "\n",
    "                self.add_metric(month_loss, name=f'month_forecasting_loss', aggregation='mean')\n",
    "\n",
    "\n",
    "            def month_sequence(self, first_level_state, inputs_extra):\n",
    "                month_forecast = self.month_sequence(first_level_state[:, -1, :])\n",
    "                month_loss = tf.keras.losses.MeanSquaredError()(month_forecast, self.get_mode(inputs_extra[:, :, 1], axis=1))\n",
    "\n",
    "                self.add_metric(month_loss, name=f'month_sequence_loss', aggregation='mean')                \n",
    "\n",
    "            def swap_task(self, inputs):\n",
    "\n",
    "                x_swap = tf.identity(inputs)\n",
    "\n",
    "                idx = tf.tile([tf.range(0, window)], (tf.shape(x_swap)[0], 1))\n",
    "\n",
    "                swap_index = tf.random.uniform((tf.shape(x_swap)[0], 1), 0, len(self.swap_combinations), dtype=tf.int64)\n",
    "\n",
    "                swap_index_onehot = tf.one_hot(swap_index, len(self.swap_combinations))\n",
    "\n",
    "                swap = tf.gather(tf.convert_to_tensor(self.swap_combinations), swap_index)\n",
    "\n",
    "                swap_numpy = swap.numpy()\n",
    "                idx_numpy = idx.numpy()\n",
    "                idx_numpy = np.concatenate((idx_numpy.reshape(idx_numpy.shape[0], 4, -1)[np.arange(idx_numpy.shape[0]), swap_numpy[..., 0].squeeze()],\n",
    "                            idx_numpy.reshape(idx_numpy.shape[0], 4, -1)[np.arange(idx_numpy.shape[0]), swap_numpy[..., 1].squeeze()],\n",
    "                            idx_numpy.reshape(idx_numpy.shape[0], 4, -1)[np.arange(idx_numpy.shape[0]), swap_numpy[..., 2].squeeze()],\n",
    "                            idx_numpy.reshape(idx_numpy.shape[0], 4, -1)[np.arange(idx_numpy.shape[0]), swap_numpy[..., 3].squeeze()]), axis=1)\n",
    "\n",
    "                idx = tf.convert_to_tensor(idx_numpy)\n",
    "                x_swap = tf.gather(x_swap, idx, axis=1, batch_dims=1)\n",
    "\n",
    "                x_swap_embedding = self.embedding(x_swap)\n",
    "                swap_state = self.first_level_lstm_layer(x_swap_embedding)\n",
    "                swap_state = self.swap_lstm_layer(swap_state)\n",
    "                swap_output = self.swap_output(swap_state)\n",
    "\n",
    "                swap_loss = tf.keras.losses.CategoricalCrossentropy()(tf.squeeze(swap_index_onehot), swap_output)\n",
    "\n",
    "                self.add_metric(swap_loss, name='swap_loss', aggregation='mean')\n",
    "\n",
    "                return swap_output, swap_state\n",
    "\n",
    "            def train_step(self, data):\n",
    "                inputs, labels = data\n",
    "                gradients = []\n",
    "                \n",
    "                with tf.GradientTape(persistent=True) as tape:\n",
    "                    prediction, losses = self(inputs, True)\n",
    "                    loss = self.compiled_loss(labels, prediction, regularization_losses=self.losses)\n",
    "\n",
    "                self.compiled_metrics.update_state(labels, prediction)\n",
    "                \n",
    "                model_layers = np.array(self.layers)\n",
    "\n",
    "                # First level loss\n",
    "                first_level_trainable_weights = [w for layer in model_layers[model.global_first_level_layers] for w in layer.trainable_weights]    \n",
    "                main_grad_first_level = tape.gradient(loss, first_level_trainable_weights)\n",
    "                self.main_grad_first_level_average = update_smooth_grad(main_grad_first_level, self.main_grad_first_level_average, grad_smooth_alpha)\n",
    "\n",
    "                first_level_gradients = []\n",
    "                for l in losses[:2]:\n",
    "\n",
    "                    aux_grad = tape.gradient(l, first_level_trainable_weights)\n",
    "                    aux_grad = combined_grads(main_grad_first_level, self.main_grad_first_level_average, aux_grad, mode, overall, lamb1)\n",
    "\n",
    "                    first_level_gradients.append(aux_grad)\n",
    "\n",
    "                    gradients.extend(list(zip(aux_grad, first_level_trainable_weights)))\n",
    "\n",
    "                first_level_aux_grad = combined_grads(first_level_gradients[0], None, first_level_gradients[1], 'Multitask', overall, 1)\n",
    "                self.first_level_aux_grad_average = update_smooth_grad(first_level_aux_grad, self.first_level_aux_grad_average, grad_smooth_alpha)\n",
    "\n",
    "                #Second level loss\n",
    "                first_level_trainable_weights = [w for layer in model_layers[self.global_first_level_layers] for w in layer.trainable_weights]    \n",
    "\n",
    "                for l in losses[2:]:\n",
    "                    aux_grad = tape.gradient(l, first_level_trainable_weights)\n",
    "                    aux_grad = combined_grads(first_level_aux_grad, self.first_level_aux_grad_average, aux_grad, mode, overall, lamb2)\n",
    "                    gradients.extend(list(zip(aux_grad, first_level_trainable_weights)))\n",
    "\n",
    "                second_level_trainable_weights = [w for layer in model_layers[self.global_second_level_layers] for w in layer.trainable_weights]    \n",
    "                main_grad_second_level = tape.gradient(loss, second_level_trainable_weights)\n",
    "                self.main_grad_second_level_average = update_smooth_grad(main_grad_second_level, self.main_grad_second_level_average, grad_smooth_alpha)\n",
    "\n",
    "                for l in losses[2:]:\n",
    "                    aux_grad = tape.gradient(l, second_level_trainable_weights)\n",
    "                    aux_grad = combined_grads(main_grad_second_level, self.main_grad_second_level_average, aux_grad, mode, overall, lamb2)\n",
    "\n",
    "                    gradients.extend(list(zip(aux_grad, second_level_trainable_weights)))\n",
    "\n",
    "                # All losses\n",
    "                rest_layers = sum([self.global_second_level_layers, self.global_first_level_layers], [])\n",
    "\n",
    "                mask = np.ones(len(model_layers), bool)\n",
    "                mask[rest_layers] = False\n",
    "                all_level_trainable_weights = [w for layer in model_layers[mask] for w in layer.trainable_weights]    \n",
    "                main_grad_rest_layers = tape.gradient(loss, all_level_trainable_weights)\n",
    "\n",
    "                for l in losses:\n",
    "                    grad = tape.gradient(l, all_level_trainable_weights)\n",
    "                    gradients.extend(list(zip(grad, all_level_trainable_weights)))\n",
    "                    \n",
    "                gradients.extend(list(zip(main_grad_rest_layers, all_level_trainable_weights)))\n",
    "                #Apply all gradients\n",
    "                self.optimizer.apply_gradients(gradients)\n",
    "\n",
    "                return {m.name: m.result() for m in self.metrics}\n",
    "            \n",
    "            def call(self, inputs, training):\n",
    "                labels, labels_extra = inputs[1]\n",
    "                inputs, inputs_extra = inputs[0]\n",
    "\n",
    "                x = self.global_embedding_1(inputs)\n",
    "                x = self.global_embedding_2(x)\n",
    "                #first_level_state = self.first_level_lstm_layer(x)\n",
    "\n",
    "                # First level\n",
    "                gap_filling_outputs, gap_loss, x_gap_embedding = self.gap_filling_task(inputs)\n",
    "                noise_reduction_outputs, noise_loss, x_noise_embedding = self.noise_reduction_task(inputs)\n",
    "                #swap_output, swap_state = self.swap_task(inputs)\n",
    "                \n",
    "                x = tf.stop_gradient(tf.identity(tf.keras.layers.concatenate([x, x_gap_embedding, x_noise_embedding])))\n",
    "                \n",
    "                #Second level\n",
    "                second_level_state = self.second_level_lstm_layer(x)\n",
    "                smooth_forecasting_8, smooth_state_8, smooth_loss_8 = self.smooth_forecast_task(second_level_state, labels, 8)\n",
    "                smooth_forecasting_6, smooth_state_6, smooth_loss_6 = self.smooth_forecast_task(second_level_state, labels, 6)\n",
    "                smooth_forecasting_3, smooth_state_3, smooth_loss_3 = self.smooth_forecast_task(second_level_state, labels, 3)\n",
    "                \n",
    "                first_step_forecast, first_step_state, first_step_loss = self.one_step_forecast_task(second_level_state, labels, 0)\n",
    "                mid_step_forecast, mid_step_state, mid_step_loss = self.one_step_forecast_task(second_level_state, labels, self.horizon//2)\n",
    "                last_step_forecast, last_step_state, last_step_loss = self.one_step_forecast_task(second_level_state, labels, self.horizon-1)\n",
    "                \n",
    "                losses = [gap_loss, noise_loss, first_step_loss, mid_step_loss, last_step_loss, smooth_loss_8, smooth_loss_6, smooth_loss_3]\n",
    "                # \n",
    "                \n",
    "                tasks_loss = 0\n",
    "                for i, loss in enumerate(losses):\n",
    "                    losses[i] = (0.5/self.loss_weigts[i]**2)*loss + tf.math.log(1+self.loss_weigts[i]**2)\n",
    "                    \n",
    "                #self.add_loss(tasks_loss)\n",
    "                # Last level\n",
    "                prediction_level_state = self.prediction_level_lstm_layer(tf.stop_gradient(tf.identity(second_level_state)))\n",
    "                prediction_state = tf.keras.layers.concatenate([prediction_level_state, \n",
    "                                                                tf.stop_gradient(tf.identity(last_step_state)), \n",
    "                                                                tf.stop_gradient(tf.identity(mid_step_state)), \n",
    "                                                                tf.stop_gradient(tf.identity(first_step_state)), \n",
    "                                                                tf.stop_gradient(tf.identity(smooth_state_3))\n",
    "                                                                , tf.stop_gradient(tf.identity(smooth_state_6)), \n",
    "                                                                tf.stop_gradient(tf.identity(smooth_state_8))])\n",
    "                \n",
    "                prediction_output = self.prediction_output(prediction_state)\n",
    "                \n",
    "                if training:\n",
    "                    return prediction_output, losses\n",
    "                else:\n",
    "                    return prediction_output\n",
    "\n",
    "\n",
    "        model = MultitaskHLNet(d_model, n_features, window, horizon, batch_size, grad_smooth_alpha, lamb1, lamb2, mode, overall)\n",
    "\n",
    "        loss = 'mse'\n",
    "    \n",
    "    elif model_type == 'multitaskhlnet_taskslstm_firstlevelcnn_gradsprojected_embeddingappended_lossweighted':\n",
    "    #pred_index = -1\n",
    "\n",
    "        class MultitaskHLNet(tf.keras.Model):\n",
    "\n",
    "            def __init__(self, d_model, n_features, window, horizon, batch_size, grad_smooth_alpha, lamb1, lamb2, mode, overall):\n",
    "                super(MultitaskHLNet, self).__init__()\n",
    "                self.d_model = d_model\n",
    "                self.n_features = n_features\n",
    "                self.window = window\n",
    "                self.horizon = horizon\n",
    "                self.batch_size = batch_size\n",
    "                self.main_grad_first_level_average = None\n",
    "                self.main_grad_second_level_average = None\n",
    "                self.first_level_aux_grad_average = None\n",
    "                self.grad_smooth_alpha = grad_smooth_alpha\n",
    "                self.lamb1 = lamb1\n",
    "                self.lamb2 = lamb2\n",
    "                self.mode = mode\n",
    "                self.overall = overall\n",
    "                \n",
    "                self.loss_weights = tf.Variable(tf.ones([8]))\n",
    "                \n",
    "                self.global_embedding_1 = tf.keras.layers.Conv1D(d_model//2, 3, activation='relu', input_shape=(window, 3), padding='causal')\n",
    "                self.global_embedding_2 = tf.keras.layers.Conv1D(d_model, 3, activation='relu', padding='causal')\n",
    "                \n",
    "                self.global_first_level_layers = [1,2]\n",
    "                \"\"\"\n",
    "                    First Level tasks - Sequence featurization\n",
    "                \"\"\"\n",
    "                self.decode_cnn_1 = tf.keras.layers.Conv1DTranspose(d_model, 3, activation='relu', padding='same')\n",
    "                self.decode_cnn_2 = tf.keras.layers.Conv1DTranspose(d_model//2, 3, activation='relu', padding='same')\n",
    "                self.gap_filling_output = tf.keras.layers.Dense(n_features)\n",
    "\n",
    "                self.gaussian_noise = tf.keras.layers.GaussianNoise(0.01)\n",
    "                self.noise_reduction_output = tf.keras.layers.Dense(n_features)\n",
    "\n",
    "                #self.swap_combinations = list(itertools.permutations(np.arange(0, 4), 4))\n",
    "                #self.swap_lstm_layer = tf.keras.layers.LSTM(d_model, name='swap_lstm', activation='relu')\n",
    "                #self.swap_output = tf.keras.layers.Dense(len(self.swap_combinations), name='swap_task', activation='softmax')\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                    Second Level tasks - Forecasting helpers\n",
    "                \"\"\"\n",
    "                self.second_level_lstm_layer = tf.keras.layers.LSTM(d_model, return_sequences=True)\n",
    "\n",
    "                self.smooth_lstm_layers = {'8': tf.keras.layers.LSTM(d_model, name='smooth_forecasting_8_lstm', activation='relu'),\n",
    "                                '6':  tf.keras.layers.LSTM(d_model, name='smooth_forecasting_6_lstm', activation='relu'),\n",
    "                                '3':  tf.keras.layers.LSTM(d_model, name='smooth_forecasting_3_lstm', activation='relu')}\n",
    "\n",
    "                self.smooth_layers = {'8': tf.keras.layers.Dense(horizon-8+1, name='smooth_forecasting_8'),\n",
    "                                '6': tf.keras.layers.Dense(horizon-6+1, name='smooth_forecasting_6'),\n",
    "                                '3': tf.keras.layers.Dense(horizon-3+1, name='smooth_forecasting_3')}\n",
    "\n",
    "\n",
    "                self.one_step_forecast_lstm_layers = {'0': tf.keras.layers.LSTM(d_model, name='next_step_forecasting_lstm', activation='relu'),\n",
    "                                str(horizon//2):  tf.keras.layers.LSTM(d_model, name='mid_step_forecasting_lstm', activation='relu'),\n",
    "                                str(horizon-1):  tf.keras.layers.LSTM(d_model, name='last_step_forecasting_lstm', activation='relu')}\n",
    "\n",
    "                self.one_step_forecast = {'0': tf.keras.layers.Dense(1, name='next_step_forecasting'),\n",
    "                                    str(horizon//2): tf.keras.layers.Dense(1, name='mid_step_forecasting'),\n",
    "                                    str(horizon-1): tf.keras.layers.Dense(1, name='last_step_forecasting')}\n",
    "                self.global_second_level_layers = [8]\n",
    "                \"\"\"\n",
    "                    Last Level tasks\n",
    "                \"\"\"\n",
    "                self.prediction_level_lstm_layer = tf.keras.layers.LSTM(d_model, return_sequences=False)\n",
    "                self.prediction_output = tf.keras.layers.Dense(horizon, name='prediction')\n",
    "\n",
    "            def get_mode(self, x, axis=1):\n",
    "                dt = x.dtype\n",
    "                # Shift input in case it has negative values\n",
    "                m = tf.math.reduce_min(x)\n",
    "                x2 = x - m\n",
    "                # minlength should not be necessary but may fail without it\n",
    "                # (reported here https://github.com/tensorflow/probability/issues/962)\n",
    "                c = tfp.stats.count_integers(x2, axis=axis, dtype=dt,\n",
    "                                             minlength=tf.math.reduce_max(x2) + 1)\n",
    "                # Find the values with largest counts\n",
    "                idx = tf.math.argmax(c, axis=0, output_type=dt)\n",
    "                # Get the modes by shifting by the subtracted minimum\n",
    "                modes = idx + m\n",
    "                # Get the number of counts\n",
    "                counts = tf.math.reduce_max(c, axis=0)\n",
    "\n",
    "                return modes\n",
    "\n",
    "            def gap_filling_task(self, inputs):\n",
    "\n",
    "                #Gap filling task\n",
    "                batch_indexes = tf.tile(tf.range(tf.shape(inputs)[0])[:, tf.newaxis, tf.newaxis], (1, self.window, 1))\n",
    "                head_indexes = tf.tile(tf.range(self.window)[tf.newaxis, :, tf.newaxis], (tf.shape(inputs)[0], 1, 1))\n",
    "                feat_index = tf.random.uniform((tf.shape(inputs)[0],window,1), minval=0, maxval=3, dtype=tf.int32)\n",
    "\n",
    "                idx = tf.squeeze(tf.stack(values=[batch_indexes, head_indexes, feat_index], axis=-1))\n",
    "                idx = tf.transpose(idx, perm=(1,2,0))    \n",
    "\n",
    "                gap_index =  tf.reshape(tf.transpose(tf.random.shuffle(idx), perm= (2, 0, 1))[:, :1, :], (-1, 3))\n",
    "\n",
    "                x_gap = tf.identity(inputs)\n",
    "                x_gap_updated = tf.tensor_scatter_nd_update(x_gap, indices = gap_index, updates = -tf.ones(gap_index.shape[0])*100)\n",
    "                x_gap_embedding = self.global_embedding_1(x_gap_updated)\n",
    "                x_gap_embedding = self.global_embedding_2(x_gap_embedding)\n",
    "                x_gap_embedding = self.decode_cnn_1(x_gap_embedding)\n",
    "                x_gap_embedding = self.decode_cnn_2(x_gap_embedding)\n",
    "                #gap_state = self.first_level_lstm_layer(x_gap_embedding)\n",
    "                gap_filling_output = self.gap_filling_output(x_gap_embedding)\n",
    "                #gap_filling_output = self.gap_filling_reshape(gap_filling_output_flat)\n",
    "                #gap_filling_true = tf.reshape(tf.gather_nd(x_gap, gap_index), (tf.shape(x_gap)[0], 1))\n",
    "                gap_loss = tf.keras.losses.MeanSquaredError()(gap_filling_output, x_gap)\n",
    "\n",
    "                self.add_metric(gap_loss, name='gap_loss', aggregation='mean')\n",
    "                \n",
    "\n",
    "                return gap_filling_output, gap_loss, x_gap_embedding\n",
    "\n",
    "            def noise_reduction_task(self, inputs):\n",
    "\n",
    "                input_copy = tf.identity(inputs)\n",
    "                x_noise = self.gaussian_noise(input_copy)\n",
    "\n",
    "                x_noise_embedding = self.global_embedding_1(x_noise)\n",
    "                x_noise_embedding = self.global_embedding_2(x_noise_embedding)\n",
    "                x_noise_embedding = self.decode_cnn_1(x_noise_embedding)\n",
    "                x_noise_embedding = self.decode_cnn_2(x_noise_embedding)\n",
    "                #noise_state = self.first_level_lstm_layer(x_noise_embedding)\n",
    "                #noise_state = self.noise_reduction_lstm_layer(noise_state)\n",
    "                noise_output = self.noise_reduction_output(x_noise_embedding)\n",
    "                #noise_output = self.noise_reduction_reshape(noise_output_flat)\n",
    "\n",
    "                noise_loss = tf.keras.losses.MeanSquaredError()(noise_output, input_copy)\n",
    "\n",
    "                self.add_metric(noise_loss, name='noise_loss', aggregation='mean')\n",
    "                \n",
    "\n",
    "                return noise_output, noise_loss, x_noise_embedding\n",
    "\n",
    "            def smooth_forecast_task(self, second_level_state, labels, smooth=6):\n",
    "\n",
    "                smooth_state = self.smooth_lstm_layers[str(smooth)](second_level_state)\n",
    "                smooth_forecasting = self.smooth_layers[str(smooth)](smooth_state)\n",
    "                labels_smooth = tf.math.reduce_mean(tf.signal.frame(labels, smooth, 1, axis=1), axis=2)\n",
    "\n",
    "                smooth_loss = tf.keras.losses.MeanSquaredError()(smooth_forecasting, labels_smooth)\n",
    "\n",
    "                self.add_metric(smooth_loss, name=f'smooth_{smooth}_loss', aggregation='mean')\n",
    "                \n",
    "\n",
    "                return smooth_forecasting, smooth_state, smooth_loss\n",
    "\n",
    "            def one_step_forecast_task(self,second_level_state, labels, step=0):\n",
    "                step_state = self.one_step_forecast_lstm_layers[str(step)](second_level_state)\n",
    "                step_forecast = self.one_step_forecast[str(step)](step_state)\n",
    "                step_loss = tf.keras.losses.MeanSquaredError()(step_forecast, labels[:, step, :])\n",
    "\n",
    "                self.add_metric(step_loss, name=f'step_{step}_loss', aggregation='mean')\n",
    "                \n",
    "                \n",
    "                return step_forecast, step_state, step_loss\n",
    "\n",
    "            def day_of_week_forecasting(self, second_level_state, labels_extra):\n",
    "                week_forecast = self.day_of_week_forecasting(second_level_state[:, -1, :])\n",
    "                week_loss = tf.keras.losses.MeanSquaredError()(week_forecast, self.get_mode(labels_extra[:, :, 0], axis=1))\n",
    "\n",
    "                self.add_metric(week_loss, name=f'week_forecasting_loss', aggregation='mean')\n",
    "\n",
    "            def day_of_week_sequence(self, first_level_state, inputs_extra):\n",
    "                # TODO: ONEHOT\n",
    "                week_forecast = self.day_of_week_sequence(first_level_state[:, -1, :])\n",
    "                week_loss = tf.keras.losses.MeanSquaredError()(week_forecast, self.get_mode(inputs_extra[:, :, 0], axis=1))\n",
    "\n",
    "                self.add_metric(week_loss, name=f'week_sequence_loss', aggregation='mean') \n",
    "\n",
    "            def quarter_forecasting(self, second_level_state, labels_extra):\n",
    "                quarter_forecast = self.quarter_forecasting(second_level_state[:, -1, :])\n",
    "                quarter_loss = tf.keras.losses.MeanSquaredError()(quarter_forecast, self.get_mode(labels_extra[:, :, 2], axis=1))\n",
    "\n",
    "                self.add_metric(quarter_loss, name=f'quarter_forecasting_loss', aggregation='mean')\n",
    "\n",
    "            def quarter_sequence(self, first_level_state, inputs_extra):\n",
    "                quarter_forecast = self.quarter_sequence(first_level_state[:, -1, :])\n",
    "                quarter_loss = tf.keras.losses.MeanSquaredError()(quarter_forecast, self.get_mode(inputs_extra[:, :, 2], axis=1))\n",
    "\n",
    "                self.add_metric(quarter_loss, name=f'quarter_sequence_loss', aggregation='mean') \n",
    "\n",
    "\n",
    "            def month_forecasting(self, second_level_state, labels_extra):\n",
    "                month_forecast = self.month_forecasting(second_level_state[:, -1, :])\n",
    "                month_loss = tf.keras.losses.MeanSquaredError()(month_forecast, self.get_mode(labels_extra[:, :, 1], axis=1))\n",
    "\n",
    "                self.add_metric(month_loss, name=f'month_forecasting_loss', aggregation='mean')\n",
    "\n",
    "\n",
    "            def month_sequence(self, first_level_state, inputs_extra):\n",
    "                month_forecast = self.month_sequence(first_level_state[:, -1, :])\n",
    "                month_loss = tf.keras.losses.MeanSquaredError()(month_forecast, self.get_mode(inputs_extra[:, :, 1], axis=1))\n",
    "\n",
    "                self.add_metric(month_loss, name=f'month_sequence_loss', aggregation='mean')                \n",
    "\n",
    "            def swap_task(self, inputs):\n",
    "\n",
    "                x_swap = tf.identity(inputs)\n",
    "\n",
    "                idx = tf.tile([tf.range(0, window)], (tf.shape(x_swap)[0], 1))\n",
    "\n",
    "                swap_index = tf.random.uniform((tf.shape(x_swap)[0], 1), 0, len(self.swap_combinations), dtype=tf.int64)\n",
    "\n",
    "                swap_index_onehot = tf.one_hot(swap_index, len(self.swap_combinations))\n",
    "\n",
    "                swap = tf.gather(tf.convert_to_tensor(self.swap_combinations), swap_index)\n",
    "\n",
    "                swap_numpy = swap.numpy()\n",
    "                idx_numpy = idx.numpy()\n",
    "                idx_numpy = np.concatenate((idx_numpy.reshape(idx_numpy.shape[0], 4, -1)[np.arange(idx_numpy.shape[0]), swap_numpy[..., 0].squeeze()],\n",
    "                            idx_numpy.reshape(idx_numpy.shape[0], 4, -1)[np.arange(idx_numpy.shape[0]), swap_numpy[..., 1].squeeze()],\n",
    "                            idx_numpy.reshape(idx_numpy.shape[0], 4, -1)[np.arange(idx_numpy.shape[0]), swap_numpy[..., 2].squeeze()],\n",
    "                            idx_numpy.reshape(idx_numpy.shape[0], 4, -1)[np.arange(idx_numpy.shape[0]), swap_numpy[..., 3].squeeze()]), axis=1)\n",
    "\n",
    "                idx = tf.convert_to_tensor(idx_numpy)\n",
    "                x_swap = tf.gather(x_swap, idx, axis=1, batch_dims=1)\n",
    "\n",
    "                x_swap_embedding = self.embedding(x_swap)\n",
    "                swap_state = self.first_level_lstm_layer(x_swap_embedding)\n",
    "                swap_state = self.swap_lstm_layer(swap_state)\n",
    "                swap_output = self.swap_output(swap_state)\n",
    "\n",
    "                swap_loss = tf.keras.losses.CategoricalCrossentropy()(tf.squeeze(swap_index_onehot), swap_output)\n",
    "\n",
    "                self.add_metric(swap_loss, name='swap_loss', aggregation='mean')\n",
    "\n",
    "                return swap_output, swap_state\n",
    "\n",
    "            def train_step(self, data):\n",
    "                inputs, labels = data\n",
    "                gradients = []\n",
    "                \n",
    "                with tf.GradientTape(persistent=True) as tape:\n",
    "                    prediction, losses = self(inputs, True)\n",
    "                    loss = self.compiled_loss(labels, prediction, regularization_losses=self.losses)\n",
    "\n",
    "                self.compiled_metrics.update_state(labels, prediction)\n",
    "                \n",
    "                model_layers = np.array(self.layers)\n",
    "\n",
    "                # First level loss\n",
    "                first_level_trainable_weights = [w for layer in model_layers[model.global_first_level_layers] for w in layer.trainable_weights]    \n",
    "                main_grad_first_level = tape.gradient(loss, first_level_trainable_weights)\n",
    "                self.main_grad_first_level_average = update_smooth_grad(main_grad_first_level, self.main_grad_first_level_average, grad_smooth_alpha)\n",
    "\n",
    "                first_level_gradients = []\n",
    "                for i, l in enumerate(losses[:2]):\n",
    "\n",
    "                    aux_grad = tape.gradient(l, first_level_trainable_weights)\n",
    "                    aux_grad = combined_grads(main_grad_first_level, self.main_grad_first_level_average, aux_grad, mode, overall, self.loss_weights[i])\n",
    "\n",
    "                    first_level_gradients.append(aux_grad)\n",
    "\n",
    "                    gradients.extend(list(zip(aux_grad, first_level_trainable_weights)))\n",
    "\n",
    "                first_level_aux_grad = combined_grads(first_level_gradients[0], None, first_level_gradients[1], 'Multitask', overall, 1)\n",
    "                self.first_level_aux_grad_average = update_smooth_grad(first_level_aux_grad, self.first_level_aux_grad_average, grad_smooth_alpha)\n",
    "\n",
    "                #Second level loss\n",
    "                first_level_trainable_weights = [w for layer in model_layers[self.global_first_level_layers] for w in layer.trainable_weights]    \n",
    "\n",
    "                for l in losses[2:]:\n",
    "                    aux_grad = tape.gradient(l, first_level_trainable_weights)\n",
    "                    aux_grad = combined_grads(first_level_aux_grad, self.first_level_aux_grad_average, aux_grad, mode, overall, self.loss_weights[i+2])\n",
    "                    gradients.extend(list(zip(aux_grad, first_level_trainable_weights)))\n",
    "\n",
    "                second_level_trainable_weights = [w for layer in model_layers[self.global_second_level_layers] for w in layer.trainable_weights]    \n",
    "                main_grad_second_level = tape.gradient(loss, second_level_trainable_weights)\n",
    "                self.main_grad_second_level_average = update_smooth_grad(main_grad_second_level, self.main_grad_second_level_average, grad_smooth_alpha)\n",
    "\n",
    "                for l in losses[2:]:\n",
    "                    aux_grad = tape.gradient(l, second_level_trainable_weights)\n",
    "                    aux_grad = combined_grads(main_grad_second_level, self.main_grad_second_level_average, aux_grad, mode, overall, self.loss_weights[i+2])\n",
    "\n",
    "                    gradients.extend(list(zip(aux_grad, second_level_trainable_weights)))\n",
    "\n",
    "                # All losses\n",
    "                rest_layers = sum([self.global_second_level_layers, self.global_first_level_layers], [])\n",
    "\n",
    "                mask = np.ones(len(model_layers), bool)\n",
    "                mask[rest_layers] = False\n",
    "                all_level_trainable_weights = [w for layer in model_layers[mask] for w in layer.trainable_weights]    \n",
    "                main_grad_rest_layers = tape.gradient(loss, all_level_trainable_weights)\n",
    "\n",
    "                for l in losses:\n",
    "                    grad = tape.gradient(l, all_level_trainable_weights)\n",
    "                    gradients.extend(list(zip(grad, all_level_trainable_weights)))\n",
    "                    \n",
    "                gradients.extend(list(zip(main_grad_rest_layers, all_level_trainable_weights)))\n",
    "                #Apply all gradients\n",
    "                self.optimizer.apply_gradients(gradients)\n",
    "\n",
    "                return {m.name: m.result() for m in self.metrics}\n",
    "            \n",
    "            def call(self, inputs, training):\n",
    "                labels, labels_extra = inputs[1]\n",
    "                inputs, inputs_extra = inputs[0]\n",
    "\n",
    "                x = self.global_embedding_1(inputs)\n",
    "                x = self.global_embedding_2(x)\n",
    "                #first_level_state = self.first_level_lstm_layer(x)\n",
    "\n",
    "                # First level\n",
    "                gap_filling_outputs, gap_loss, x_gap_embedding = self.gap_filling_task(inputs)\n",
    "                noise_reduction_outputs, noise_loss, x_noise_embedding = self.noise_reduction_task(inputs)\n",
    "                #swap_output, swap_state = self.swap_task(inputs)\n",
    "                \n",
    "                x = tf.keras.layers.concatenate([x, x_gap_embedding, x_noise_embedding])\n",
    "                \n",
    "                #Second level\n",
    "                second_level_state = self.second_level_lstm_layer(x)\n",
    "                smooth_forecasting_8, smooth_state_8, smooth_loss_8 = self.smooth_forecast_task(second_level_state, labels, 8)\n",
    "                smooth_forecasting_6, smooth_state_6, smooth_loss_6 = self.smooth_forecast_task(second_level_state, labels, 6)\n",
    "                smooth_forecasting_3, smooth_state_3, smooth_loss_3 = self.smooth_forecast_task(second_level_state, labels, 3)\n",
    "                \n",
    "                first_step_forecast, first_step_state, first_step_loss = self.one_step_forecast_task(second_level_state, labels, 0)\n",
    "                mid_step_forecast, mid_step_state, mid_step_loss = self.one_step_forecast_task(second_level_state, labels, self.horizon//2)\n",
    "                last_step_forecast, last_step_state, last_step_loss = self.one_step_forecast_task(second_level_state, labels, self.horizon-1)\n",
    "                \n",
    "                losses = [gap_loss, noise_loss, first_step_loss, mid_step_loss, last_step_loss, smooth_loss_8, smooth_loss_6, smooth_loss_3]\n",
    "                # \n",
    "                \n",
    "                \"\"\"tasks_loss = 0\n",
    "                for i, loss in enumerate(losses):\n",
    "                    losses[i] = (0.5/self.loss_weigts[i]**2)*loss + tf.math.log(1+self.loss_weigts[i]**2)\n",
    "                    \"\"\"\n",
    "                #self.add_loss(tasks_loss)\n",
    "                # Last level\n",
    "                prediction_level_state = self.prediction_level_lstm_layer(second_level_state)\n",
    "                prediction_state = tf.keras.layers.concatenate([prediction_level_state, last_step_state, mid_step_state, \n",
    "                                         first_step_state, smooth_state_3, smooth_state_6, smooth_state_8])\n",
    "                \n",
    "                prediction_output = self.prediction_output(prediction_state)\n",
    "                \n",
    "                if training:\n",
    "                    return prediction_output, losses\n",
    "                else:\n",
    "                    return prediction_output\n",
    "\n",
    "\n",
    "        model = MultitaskHLNet(d_model, n_features, window, horizon, batch_size, grad_smooth_alpha, lamb1, lamb2, mode, overall)\n",
    "\n",
    "        loss = 'mse'\n",
    "    \n",
    "    elif model_type == 'multitaskhlnet_taskslstm_firstlevelcnn_gradsprojected_embeddingappended_swapappended':\n",
    "    #pred_index = -1\n",
    "\n",
    "        class MultitaskHLNet(tf.keras.Model):\n",
    "\n",
    "            def __init__(self, d_model, n_features, window, horizon, batch_size, grad_smooth_alpha, lamb1, lamb2, mode, overall):\n",
    "                super(MultitaskHLNet, self).__init__()\n",
    "                self.d_model = d_model\n",
    "                self.n_features = n_features\n",
    "                self.window = window\n",
    "                self.horizon = horizon\n",
    "                self.batch_size = batch_size\n",
    "                self.main_grad_first_level_average = None\n",
    "                self.main_grad_second_level_average = None\n",
    "                self.first_level_aux_grad_average = None\n",
    "                self.grad_smooth_alpha = grad_smooth_alpha\n",
    "                self.lamb1 = lamb1\n",
    "                self.lamb2 = lamb2\n",
    "                self.mode = mode\n",
    "                self.overall = overall\n",
    "                \n",
    "                self.loss_weigts = tf.Variable(tf.ones([8]))\n",
    "                \n",
    "                self.global_embedding_1 = tf.keras.layers.Conv1D(d_model//2, 3, activation='relu', input_shape=(window, 3), padding='causal')\n",
    "                self.global_embedding_2 = tf.keras.layers.Conv1D(d_model, 3, activation='relu', padding='causal')\n",
    "                \n",
    "                self.global_first_level_layers = [0,1]\n",
    "                \"\"\"\n",
    "                    First Level tasks - Sequence featurization\n",
    "                \"\"\"\n",
    "                self.decode_cnn_1 = tf.keras.layers.Conv1DTranspose(d_model, 3, activation='relu', padding='same')\n",
    "                self.decode_cnn_2 = tf.keras.layers.Conv1DTranspose(d_model//2, 3, activation='relu', padding='same')\n",
    "                self.gap_filling_output = tf.keras.layers.Dense(n_features)\n",
    "\n",
    "                self.gaussian_noise = tf.keras.layers.GaussianNoise(0.01)\n",
    "                self.noise_reduction_output = tf.keras.layers.Dense(n_features)\n",
    "\n",
    "                self.swap_combinations = list(itertools.permutations(np.arange(0, 4), 4))\n",
    "                self.swap_lstm_layer = tf.keras.layers.LSTM(d_model, name='swap_lstm', activation='relu')\n",
    "                self.swap_output = tf.keras.layers.Dense(len(self.swap_combinations), name='swap_task', activation='softmax')\n",
    "\n",
    "                \n",
    "                \"\"\"\n",
    "                    Second Level tasks - Forecasting helpers\n",
    "                \"\"\"\n",
    "                self.second_level_lstm_layer = tf.keras.layers.LSTM(d_model, return_sequences=True)\n",
    "\n",
    "                self.smooth_lstm_layers = {'8': tf.keras.layers.LSTM(d_model, name='smooth_forecasting_8_lstm', activation='relu'),\n",
    "                                '6':  tf.keras.layers.LSTM(d_model, name='smooth_forecasting_6_lstm', activation='relu'),\n",
    "                                '3':  tf.keras.layers.LSTM(d_model, name='smooth_forecasting_3_lstm', activation='relu')}\n",
    "\n",
    "                self.smooth_layers = {'8': tf.keras.layers.Dense(horizon-8+1, name='smooth_forecasting_8'),\n",
    "                                '6': tf.keras.layers.Dense(horizon-6+1, name='smooth_forecasting_6'),\n",
    "                                '3': tf.keras.layers.Dense(horizon-3+1, name='smooth_forecasting_3')}\n",
    "\n",
    "\n",
    "                self.one_step_forecast_lstm_layers = {'0': tf.keras.layers.LSTM(d_model, name='next_step_forecasting_lstm', activation='relu'),\n",
    "                                str(horizon//2):  tf.keras.layers.LSTM(d_model, name='mid_step_forecasting_lstm', activation='relu'),\n",
    "                                str(horizon-1):  tf.keras.layers.LSTM(d_model, name='last_step_forecasting_lstm', activation='relu')}\n",
    "\n",
    "                self.one_step_forecast = {'0': tf.keras.layers.Dense(1, name='next_step_forecasting'),\n",
    "                                    str(horizon//2): tf.keras.layers.Dense(1, name='mid_step_forecasting'),\n",
    "                                    str(horizon-1): tf.keras.layers.Dense(1, name='last_step_forecasting')}\n",
    "                self.global_second_level_layers = [9]\n",
    "                \"\"\"\n",
    "                    Last Level tasks\n",
    "                \"\"\"\n",
    "                self.prediction_level_lstm_layer = tf.keras.layers.LSTM(d_model, return_sequences=False)\n",
    "                self.prediction_output = tf.keras.layers.Dense(horizon, name='prediction')\n",
    "\n",
    "            def get_mode(self, x, axis=1):\n",
    "                dt = x.dtype\n",
    "                # Shift input in case it has negative values\n",
    "                m = tf.math.reduce_min(x)\n",
    "                x2 = x - m\n",
    "                # minlength should not be necessary but may fail without it\n",
    "                # (reported here https://github.com/tensorflow/probability/issues/962)\n",
    "                c = tfp.stats.count_integers(x2, axis=axis, dtype=dt,\n",
    "                                             minlength=tf.math.reduce_max(x2) + 1)\n",
    "                # Find the values with largest counts\n",
    "                idx = tf.math.argmax(c, axis=0, output_type=dt)\n",
    "                # Get the modes by shifting by the subtracted minimum\n",
    "                modes = idx + m\n",
    "                # Get the number of counts\n",
    "                counts = tf.math.reduce_max(c, axis=0)\n",
    "\n",
    "                return modes\n",
    "\n",
    "            def gap_filling_task(self, inputs):\n",
    "\n",
    "                #Gap filling task\n",
    "                batch_indexes = tf.tile(tf.range(tf.shape(inputs)[0])[:, tf.newaxis, tf.newaxis], (1, self.window, 1))\n",
    "                head_indexes = tf.tile(tf.range(self.window)[tf.newaxis, :, tf.newaxis], (tf.shape(inputs)[0], 1, 1))\n",
    "                feat_index = tf.random.uniform((tf.shape(inputs)[0],window,1), minval=0, maxval=3, dtype=tf.int32)\n",
    "\n",
    "                idx = tf.squeeze(tf.stack(values=[batch_indexes, head_indexes, feat_index], axis=-1))\n",
    "                idx = tf.transpose(idx, perm=(1,2,0))    \n",
    "\n",
    "                gap_index =  tf.reshape(tf.transpose(tf.random.shuffle(idx), perm= (2, 0, 1))[:, :1, :], (-1, 3))\n",
    "\n",
    "                x_gap = tf.identity(inputs)\n",
    "                x_gap_updated = tf.tensor_scatter_nd_update(x_gap, indices = gap_index, updates = -tf.ones(gap_index.shape[0])*100)\n",
    "                x_gap_embedding = self.global_embedding_1(x_gap_updated)\n",
    "                x_gap_embedding = self.global_embedding_2(x_gap_embedding)\n",
    "                x_gap_embedding = self.decode_cnn_1(x_gap_embedding)\n",
    "                x_gap_embedding = self.decode_cnn_2(x_gap_embedding)\n",
    "                #gap_state = self.first_level_lstm_layer(x_gap_embedding)\n",
    "                gap_filling_output = self.gap_filling_output(x_gap_embedding)\n",
    "                #gap_filling_output = self.gap_filling_reshape(gap_filling_output_flat)\n",
    "                #gap_filling_true = tf.reshape(tf.gather_nd(x_gap, gap_index), (tf.shape(x_gap)[0], 1))\n",
    "                gap_loss = tf.keras.losses.MeanSquaredError()(gap_filling_output, x_gap)\n",
    "\n",
    "                self.add_metric(gap_loss, name='gap_loss', aggregation='mean')\n",
    "                \n",
    "\n",
    "                return gap_filling_output, gap_loss, x_gap_embedding\n",
    "\n",
    "            def noise_reduction_task(self, inputs):\n",
    "\n",
    "                input_copy = tf.identity(inputs)\n",
    "                x_noise = self.gaussian_noise(input_copy)\n",
    "\n",
    "                x_noise_embedding = self.global_embedding_1(x_noise)\n",
    "                x_noise_embedding = self.global_embedding_2(x_noise_embedding)\n",
    "                x_noise_embedding = self.decode_cnn_1(x_noise_embedding)\n",
    "                x_noise_embedding = self.decode_cnn_2(x_noise_embedding)\n",
    "                #noise_state = self.first_level_lstm_layer(x_noise_embedding)\n",
    "                #noise_state = self.noise_reduction_lstm_layer(noise_state)\n",
    "                noise_output = self.noise_reduction_output(x_noise_embedding)\n",
    "                #noise_output = self.noise_reduction_reshape(noise_output_flat)\n",
    "\n",
    "                noise_loss = tf.keras.losses.MeanSquaredError()(noise_output, input_copy)\n",
    "\n",
    "                self.add_metric(noise_loss, name='noise_loss', aggregation='mean')\n",
    "                \n",
    "\n",
    "                return noise_output, noise_loss, x_noise_embedding\n",
    "\n",
    "            def smooth_forecast_task(self, second_level_state, labels, smooth=6):\n",
    "\n",
    "                smooth_state = self.smooth_lstm_layers[str(smooth)](second_level_state)\n",
    "                smooth_forecasting = self.smooth_layers[str(smooth)](smooth_state)\n",
    "                labels_smooth = tf.math.reduce_mean(tf.signal.frame(labels, smooth, 1, axis=1), axis=2)\n",
    "\n",
    "                smooth_loss = tf.keras.losses.MeanSquaredError()(smooth_forecasting, labels_smooth)\n",
    "\n",
    "                self.add_metric(smooth_loss, name=f'smooth_{smooth}_loss', aggregation='mean')\n",
    "                \n",
    "\n",
    "                return smooth_forecasting, smooth_state, smooth_loss\n",
    "\n",
    "            def one_step_forecast_task(self,second_level_state, labels, step=0):\n",
    "                step_state = self.one_step_forecast_lstm_layers[str(step)](second_level_state)\n",
    "                step_forecast = self.one_step_forecast[str(step)](step_state)\n",
    "                step_loss = tf.keras.losses.MeanSquaredError()(step_forecast, labels[:, step, :])\n",
    "\n",
    "                self.add_metric(step_loss, name=f'step_{step}_loss', aggregation='mean')\n",
    "                \n",
    "                \n",
    "                return step_forecast, step_state, step_loss\n",
    "\n",
    "            def day_of_week_forecasting(self, second_level_state, labels_extra):\n",
    "                week_forecast = self.day_of_week_forecasting(second_level_state[:, -1, :])\n",
    "                week_loss = tf.keras.losses.MeanSquaredError()(week_forecast, self.get_mode(labels_extra[:, :, 0], axis=1))\n",
    "\n",
    "                self.add_metric(week_loss, name=f'week_forecasting_loss', aggregation='mean')\n",
    "\n",
    "            def day_of_week_sequence(self, first_level_state, inputs_extra):\n",
    "                # TODO: ONEHOT\n",
    "                week_forecast = self.day_of_week_sequence(first_level_state[:, -1, :])\n",
    "                week_loss = tf.keras.losses.MeanSquaredError()(week_forecast, self.get_mode(inputs_extra[:, :, 0], axis=1))\n",
    "\n",
    "                self.add_metric(week_loss, name=f'week_sequence_loss', aggregation='mean') \n",
    "\n",
    "            def quarter_forecasting(self, second_level_state, labels_extra):\n",
    "                quarter_forecast = self.quarter_forecasting(second_level_state[:, -1, :])\n",
    "                quarter_loss = tf.keras.losses.MeanSquaredError()(quarter_forecast, self.get_mode(labels_extra[:, :, 2], axis=1))\n",
    "\n",
    "                self.add_metric(quarter_loss, name=f'quarter_forecasting_loss', aggregation='mean')\n",
    "\n",
    "            def quarter_sequence(self, first_level_state, inputs_extra):\n",
    "                quarter_forecast = self.quarter_sequence(first_level_state[:, -1, :])\n",
    "                quarter_loss = tf.keras.losses.MeanSquaredError()(quarter_forecast, self.get_mode(inputs_extra[:, :, 2], axis=1))\n",
    "\n",
    "                self.add_metric(quarter_loss, name=f'quarter_sequence_loss', aggregation='mean') \n",
    "\n",
    "\n",
    "            def month_forecasting(self, second_level_state, labels_extra):\n",
    "                month_forecast = self.month_forecasting(second_level_state[:, -1, :])\n",
    "                month_loss = tf.keras.losses.MeanSquaredError()(month_forecast, self.get_mode(labels_extra[:, :, 1], axis=1))\n",
    "\n",
    "                self.add_metric(month_loss, name=f'month_forecasting_loss', aggregation='mean')\n",
    "\n",
    "\n",
    "            def month_sequence(self, first_level_state, inputs_extra):\n",
    "                month_forecast = self.month_sequence(first_level_state[:, -1, :])\n",
    "                month_loss = tf.keras.losses.MeanSquaredError()(month_forecast, self.get_mode(inputs_extra[:, :, 1], axis=1))\n",
    "\n",
    "                self.add_metric(month_loss, name=f'month_sequence_loss', aggregation='mean')                \n",
    "\n",
    "            def swap_task(self, inputs):\n",
    "\n",
    "                x_swap = tf.identity(inputs)\n",
    "\n",
    "                idx = tf.tile([tf.range(0, window)], (tf.shape(x_swap)[0], 1))\n",
    "\n",
    "                swap_index = tf.random.uniform((tf.shape(x_swap)[0], 1), 0, len(self.swap_combinations), dtype=tf.int64)\n",
    "\n",
    "                swap_index_onehot = tf.one_hot(swap_index, len(self.swap_combinations))\n",
    "\n",
    "                swap = tf.gather(tf.convert_to_tensor(self.swap_combinations), swap_index)\n",
    "\n",
    "                swap_numpy = swap.numpy()\n",
    "                idx_numpy = idx.numpy()\n",
    "                idx_numpy = np.concatenate((idx_numpy.reshape(idx_numpy.shape[0], 4, -1)[np.arange(idx_numpy.shape[0]), swap_numpy[..., 0].squeeze()],\n",
    "                            idx_numpy.reshape(idx_numpy.shape[0], 4, -1)[np.arange(idx_numpy.shape[0]), swap_numpy[..., 1].squeeze()],\n",
    "                            idx_numpy.reshape(idx_numpy.shape[0], 4, -1)[np.arange(idx_numpy.shape[0]), swap_numpy[..., 2].squeeze()],\n",
    "                            idx_numpy.reshape(idx_numpy.shape[0], 4, -1)[np.arange(idx_numpy.shape[0]), swap_numpy[..., 3].squeeze()]), axis=1)\n",
    "\n",
    "                idx = tf.convert_to_tensor(idx_numpy)\n",
    "                x_swap = tf.gather(x_swap, idx, axis=1, batch_dims=1)\n",
    "\n",
    "                x_swap_embedding = self.embedding(x_swap)\n",
    "                swap_state = self.first_level_lstm_layer(x_swap_embedding)\n",
    "                swap_state = self.swap_lstm_layer(swap_state)\n",
    "                swap_output = self.swap_output(swap_state)\n",
    "\n",
    "                swap_loss = tf.keras.losses.CategoricalCrossentropy()(tf.squeeze(swap_index_onehot), swap_output)\n",
    "\n",
    "                self.add_metric(swap_loss, name='swap_loss', aggregation='mean')\n",
    "\n",
    "                return swap_output, swap_state, swap_loss\n",
    "\n",
    "            def train_step(self, data):\n",
    "                inputs, labels = data\n",
    "                gradients = []\n",
    "                \n",
    "                with tf.GradientTape(persistent=True) as tape:\n",
    "                    prediction, losses = self(inputs, True)\n",
    "                    loss = self.compiled_loss(labels, prediction, regularization_losses=self.losses)\n",
    "\n",
    "                self.compiled_metrics.update_state(labels, prediction)\n",
    "                \n",
    "                model_layers = np.array(self.layers)\n",
    "\n",
    "                # First level loss\n",
    "                first_level_trainable_weights = [w for layer in model_layers[model.global_first_level_layers] for w in layer.trainable_weights]    \n",
    "                main_grad_first_level = tape.gradient(loss, first_level_trainable_weights)\n",
    "                self.main_grad_first_level_average = update_smooth_grad(main_grad_first_level, self.main_grad_first_level_average, grad_smooth_alpha)\n",
    "\n",
    "                first_level_gradients = []\n",
    "                for l in losses[:2]:\n",
    "\n",
    "                    aux_grad = tape.gradient(l, first_level_trainable_weights)\n",
    "                    aux_grad = combined_grads(main_grad_first_level, self.main_grad_first_level_average, aux_grad, mode, overall, lamb1)\n",
    "\n",
    "                    first_level_gradients.append(aux_grad)\n",
    "\n",
    "                    gradients.extend(list(zip(aux_grad, first_level_trainable_weights)))\n",
    "\n",
    "                first_level_aux_grad = combined_grads(first_level_gradients[0], None, first_level_gradients[1], 'Multitask', overall, 1)\n",
    "                self.first_level_aux_grad_average = update_smooth_grad(first_level_aux_grad, self.first_level_aux_grad_average, grad_smooth_alpha)\n",
    "\n",
    "                #Second level loss\n",
    "                first_level_trainable_weights = [w for layer in model_layers[self.global_first_level_layers] for w in layer.trainable_weights]    \n",
    "\n",
    "                for l in losses[2:]:\n",
    "                    aux_grad = tape.gradient(l, first_level_trainable_weights)\n",
    "                    aux_grad = combined_grads(first_level_aux_grad, self.first_level_aux_grad_average, aux_grad, mode, overall, lamb2)\n",
    "                    gradients.extend(list(zip(aux_grad, first_level_trainable_weights)))\n",
    "\n",
    "                second_level_trainable_weights = [w for layer in model_layers[self.global_second_level_layers] for w in layer.trainable_weights]    \n",
    "                main_grad_second_level = tape.gradient(loss, second_level_trainable_weights)\n",
    "                self.main_grad_second_level_average = update_smooth_grad(main_grad_second_level, self.main_grad_second_level_average, grad_smooth_alpha)\n",
    "\n",
    "                for l in losses[2:]:\n",
    "                    aux_grad = tape.gradient(l, second_level_trainable_weights)\n",
    "                    aux_grad = combined_grads(main_grad_second_level, self.main_grad_second_level_average, aux_grad, mode, overall, lamb2)\n",
    "\n",
    "                    gradients.extend(list(zip(aux_grad, second_level_trainable_weights)))\n",
    "\n",
    "                # All losses\n",
    "                rest_layers = sum([self.global_second_level_layers, self.global_first_level_layers], [])\n",
    "\n",
    "                mask = np.ones(len(model_layers), bool)\n",
    "                mask[rest_layers] = False\n",
    "                all_level_trainable_weights = [w for layer in model_layers[mask] for w in layer.trainable_weights]    \n",
    "                main_grad_rest_layers = tape.gradient(loss, all_level_trainable_weights)\n",
    "\n",
    "                for l in losses:\n",
    "                    grad = tape.gradient(l, all_level_trainable_weights)\n",
    "                    gradients.extend(list(zip(grad, all_level_trainable_weights)))\n",
    "                    \n",
    "                gradients.extend(list(zip(main_grad_rest_layers, all_level_trainable_weights)))\n",
    "                #Apply all gradients\n",
    "                self.optimizer.apply_gradients(gradients)\n",
    "\n",
    "                return {m.name: m.result() for m in self.metrics}\n",
    "            \n",
    "            def call(self, inputs, training):\n",
    "                labels, labels_extra = inputs[1]\n",
    "                inputs, inputs_extra = inputs[0]\n",
    "\n",
    "                x = self.global_embedding_1(inputs)\n",
    "                x = self.global_embedding_2(x)\n",
    "                #first_level_state = self.first_level_lstm_layer(x)\n",
    "\n",
    "                # First level\n",
    "                gap_filling_outputs, gap_loss, x_gap_embedding = self.gap_filling_task(inputs)\n",
    "                noise_reduction_outputs, noise_loss, x_noise_embedding = self.noise_reduction_task(inputs)\n",
    "                swap_output, swap_state, swap_loss = self.swap_task(inputs)\n",
    "                \n",
    "                x = tf.keras.layers.concatenate([x, x_gap_embedding, x_noise_embedding])\n",
    "                \n",
    "                #Second level\n",
    "                second_level_state = self.second_level_lstm_layer(x)\n",
    "                smooth_forecasting_8, smooth_state_8, smooth_loss_8 = self.smooth_forecast_task(second_level_state, labels, 8)\n",
    "                smooth_forecasting_6, smooth_state_6, smooth_loss_6 = self.smooth_forecast_task(second_level_state, labels, 6)\n",
    "                smooth_forecasting_3, smooth_state_3, smooth_loss_3 = self.smooth_forecast_task(second_level_state, labels, 3)\n",
    "                \n",
    "                first_step_forecast, first_step_state, first_step_loss = self.one_step_forecast_task(second_level_state, labels, 0)\n",
    "                mid_step_forecast, mid_step_state, mid_step_loss = self.one_step_forecast_task(second_level_state, labels, self.horizon//2)\n",
    "                last_step_forecast, last_step_state, last_step_loss = self.one_step_forecast_task(second_level_state, labels, self.horizon-1)\n",
    "                \n",
    "                losses = [gap_loss, noise_loss, swap_loss, first_step_loss, mid_step_loss, last_step_loss, smooth_loss_8, smooth_loss_6, smooth_loss_3]\n",
    "                # smooth_loss_8, smooth_loss_6, smooth_loss_3\n",
    "                \n",
    "                \"\"\"tasks_loss = 0\n",
    "                for i, loss in enumerate(losses):\n",
    "                    losses[i] = (0.5/self.loss_weigts[i]**2)*loss + tf.math.log(1+self.loss_weigts[i]**2)\"\"\"\n",
    "                    \n",
    "                #self.add_loss(tasks_loss)\n",
    "                # Last level\n",
    "                prediction_level_state = self.prediction_level_lstm_layer(second_level_state)\n",
    "                prediction_state = tf.keras.layers.concatenate([prediction_level_state, last_step_state, mid_step_state, \n",
    "                                         first_step_state, smooth_state_3, smooth_state_6, smooth_state_8])\n",
    "                \n",
    "                prediction_output = self.prediction_output(prediction_state)\n",
    "                \n",
    "                if training:\n",
    "                    return prediction_output, losses\n",
    "                else:\n",
    "                    return prediction_output\n",
    "\n",
    "\n",
    "        model = MultitaskHLNet(d_model, n_features, window, horizon, batch_size, grad_smooth_alpha, lamb1, lamb2, mode, overall)\n",
    "\n",
    "        loss = 'mse'\n",
    "        \n",
    "    return model, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class hierarchical_loss(tf.keras.losses.Loss):\n",
    "\n",
    "    def __init__(self, base_criterion, gf, ge,  reduction=tf.keras.losses.Reduction.SUM):\n",
    "        self.gf = gf\n",
    "        self.ge = ge\n",
    "        self.base_criterion = base_criterion\n",
    "        self.reduction = reduction\n",
    "        self.name = 'HierarchicalLoss'\n",
    "\n",
    "    def __call__(self, y_true, y_pred, sample_weight=None):\n",
    "        criterion = tf.keras.losses.get(self.base_criterion)\n",
    "\n",
    "        y_h_true = tf.math.reduce_mean(tf.signal.frame(y_true, self.gf, self.ge, axis=1), axis=2)\n",
    "        loss = criterion(y_pred, tf.squeeze(y_h_true))\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_window(window_size, horizon_size, label_indexes, return_input_labels, n_features, features):\n",
    "    \n",
    "    inputs = features[:, :window_size, :n_features]\n",
    "    labels = features[:, window_size:, :n_features]\n",
    "\n",
    "    labels = tf.stack(\n",
    "        [labels[:, :, index] for index in label_indexes],\n",
    "        axis=-1)\n",
    "\n",
    "    inputs.set_shape([None, window_size, None])\n",
    "    labels.set_shape([None, horizon_size, None])\n",
    "\n",
    "    if features.shape[2]>n_features:\n",
    "        inputs_extra = features[:, :window_size, n_features:]\n",
    "        labels_extra = features[:, window_size:, n_features:]\n",
    "        inputs_extra.set_shape([None, window_size, None])\n",
    "        labels_extra.set_shape([None, horizon_size, None])\n",
    "        \n",
    "        inputs = (inputs, inputs_extra)\n",
    "        labels_input = (labels, labels_extra)\n",
    "         \n",
    "    if return_input_labels:\n",
    "        labels_input = labels if features.shape[2]<=n_features else labels_input\n",
    "        inputs = (inputs, labels_input)\n",
    "        \n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def censored_vector(u, v, mode='Projection'):\n",
    "    \"\"\"Adjusts the auxiliary loss gradient\n",
    "\n",
    "    Adjusts the auxiliary loss gradient before adding it to the primary loss\n",
    "    gradient and using a gradient descent-based method\n",
    "\n",
    "    Args:\n",
    "    u: A tensorflow variable representing the auxiliary loss gradient\n",
    "    v: A tensorflow variable representing the primary loss gradient\n",
    "    mode: The method used for the adjustment:\n",
    "      - Single task: the auxiliary loss gradient is ignored\n",
    "      - Multitask: the auxiliary loss gradient is kept as it is\n",
    "      - Unweighted cosine: cf. https://arxiv.org/abs/1812.02224\n",
    "      - Weighted cosine: cf. https://arxiv.org/abs/1812.02224\n",
    "      - Projection: cf. https://github.com/vivien000/auxiliary-learning\n",
    "      - Parameter-wise: same as projection but at the level of each parameter\n",
    "\n",
    "    Returns:\n",
    "    A tensorflow variable representing the adjusted auxiliary loss gradient\n",
    "    \"\"\"\n",
    "    if mode == 'Single task':\n",
    "        return 0  \n",
    "    if mode == 'Multitask':\n",
    "        return u\n",
    "    l_u, l_v = tf.norm(u), tf.norm(v)\n",
    "    if l_u.numpy() == 0 or l_v.numpy() == 0:\n",
    "        return u\n",
    "    u_dot_v = tf.math.reduce_sum(u*v)\n",
    "    if mode == 'Unweighted cosine':\n",
    "        return u if u_dot_v > 0 else tf.zeros_like(u)\n",
    "    if mode == 'Weighted cosine':\n",
    "        return tf.math.maximum(u_dot_v, 0)*u/l_u/l_v\n",
    "    if mode == 'Projection':\n",
    "        return u - tf.math.minimum(u_dot_v, 0)*v/l_v/l_v\n",
    "    if mode == 'Parameter-wise':\n",
    "        return u*((tf.math.sign(u*v)+1)/2)\n",
    "\n",
    "def combined_grads(primary_grad,\n",
    "                   average_primary_grad,\n",
    "                   auxiliary_grad,\n",
    "                   mode,\n",
    "                   overall=False,\n",
    "                   lam=1):\n",
    "    \"\"\"Combines auxiliary loss gradients and primary loss gradients\n",
    "\n",
    "    Combines a sequence of auxiliary loss gradients and a sequence of primary\n",
    "    loss gradients before performing a gradient descent step\n",
    "\n",
    "    Args:\n",
    "    primary_grad: A list of tensorflow variables corresponding to the primary\n",
    "    loss gradient for the network's Keras variables\n",
    "    average_primary_grad: A list of tensorflow variables corresponding to\n",
    "    exponential moving averages of the elements above\n",
    "    auxiliary_grad: A list of tensorflow variables corresponding to the\n",
    "    auxiliary loss gradient for the network's Keras variables\n",
    "    mode: The method used for the adjustment:\n",
    "      - Single task: the auxiliary loss gradient is ignored\n",
    "      - Multitask: the auxiliary loss gradient is kept as it is\n",
    "      - Unweighted cosine: cf. https://arxiv.org/abs/1812.02224\n",
    "      - Weighted cosine: cf. https://arxiv.org/abs/1812.02224\n",
    "      - Projection: cf. https://github.com/vivien000/auxiliary-learning\n",
    "      - Parameter-wise: same as projection but at the level of each parameter\n",
    "    overall: True if the transformation takes place at the level of the whole\n",
    "    parameter vector, i.e. the concatenation of all the Keras variables of the\n",
    "    network\n",
    "    lambda: Float balancing the primary loss and the auxiliary loss\n",
    "\n",
    "    Returns:\n",
    "    A list of tensorflow variables combining the primary loss gradients and the\n",
    "    auxiliary loss gradients and that can directly be used for the next gradient\n",
    "    descent step\n",
    "    \"\"\"\n",
    "    result = [0]*len(primary_grad)\n",
    "    a = tf.constant([], dtype=tf.float32)\n",
    "    aa = tf.constant([], dtype=tf.float32)\n",
    "    b = tf.constant([], dtype=tf.float32)\n",
    "    shapes = []\n",
    "    for i in range(len(primary_grad)):\n",
    "        if auxiliary_grad[i] is None or mode == 'Single task':\n",
    "            result[i] = primary_grad[i]\n",
    "        elif primary_grad[i] is None:\n",
    "            result[i] = lam*auxiliary_grad[i]\n",
    "        elif mode == 'Multitask':\n",
    "            result[i] = primary_grad[i] + lam*auxiliary_grad[i]\n",
    "        elif not overall:\n",
    "            if average_primary_grad is None:\n",
    "                result[i] = (primary_grad[i]\n",
    "                             + lam*censored_vector(auxiliary_grad[i],\n",
    "                                                   primary_grad[i],\n",
    "                                                   mode))\n",
    "            else:\n",
    "                result[i] = (primary_grad[i]\n",
    "                             + lam*censored_vector(auxiliary_grad[i],\n",
    "                                                   average_primary_grad[i],\n",
    "                                                   mode))\n",
    "        else:\n",
    "            a = tf.concat([a, tf.reshape(primary_grad[i], [-1])], axis=0)\n",
    "            if average_primary_grad is not None:\n",
    "                aa = tf.concat([aa, tf.reshape(average_primary_grad[i], [-1])], axis=0)\n",
    "            b = tf.concat([b, tf.reshape(auxiliary_grad[i], [-1])], axis=0)\n",
    "            shapes.append((primary_grad[i].shape,\n",
    "                         np.product(primary_grad[i].shape.as_list()),\n",
    "                         i))\n",
    "\n",
    "        if len(shapes) > 0:\n",
    "            if average_primary_grad is None:\n",
    "                c = a + lam*censored_vector(b, a, mode)\n",
    "            else:\n",
    "                c = a + lam*censored_vector(b, aa, mode)\n",
    "            start = 0\n",
    "            for i in range(len(shapes)):\n",
    "                shape, length, index = shapes[i]\n",
    "                result[index] = tf.reshape(c[start:start+length], shape)\n",
    "                start += length\n",
    "            \n",
    "    return result\n",
    "\n",
    "def update_smooth_grad(main_grad, average_grad, alpha):\n",
    "    \n",
    "    if alpha != 1:\n",
    "        if average_grad is None:\n",
    "            average_grad = main_grad\n",
    "        else:\n",
    "            for i in range(len(average_grad)):\n",
    "                if main_grad[i] is not None:\n",
    "                    average_grad[i] = ((1 - alpha)*average_grad[i]\n",
    "                                               + alpha*main_grad[i])\n",
    "    return average_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "jUODuiT7NT9h"
   },
   "outputs": [],
   "source": [
    "dataset = \"BERMEJALES\"\n",
    "target = dataset+\"-O3-AT_IN\"\n",
    "windows = [24]\n",
    "horizons = [24]\n",
    "\n",
    "batch_size=32\n",
    "lr = 3e-3\n",
    "epochs = 100\n",
    "\n",
    "d_models = [256]\n",
    "lambs = [(0.2, 0.2), (0.5, 0.5), (1, 1), (2, 2)]\n",
    "alphas = [1, 0.9]\n",
    "modes = ['Projection', 'Weighted cosine'] #, 'Unweighted cosine'\n",
    "overall = True\n",
    "\n",
    "model_types = ['multitaskhlnet_taskslstm_firstlevelcnn_gradsprojected_embeddingappended_lossweighted', 'multitaskhlnet_taskslstm_firstlevelcnn_gradsprojected_embeddingappended_swapappended']\n",
    "n_features = 3\n",
    "test_year = 2015\n",
    "\n",
    "df = pd.read_csv(\"../data/\"+dataset.lower()+\"_d.csv\")\n",
    "df = df.set_index(pd.to_datetime(df['FECHA_HORA']))\n",
    "\n",
    "df = df[df.index.year.isin([2015, 2014, 2013])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'itertools' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9cae5ae827f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpred_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhorizon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_smooth_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproduct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwindows\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhorizons\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_models\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlambs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mlamb1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlamb2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlamb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m123\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'itertools' is not defined"
     ]
    }
   ],
   "source": [
    "pred_index = None\n",
    "\n",
    "for window, horizon, d_model, model_type, lamb, grad_smooth_alpha, mode in itertools.product(windows, horizons, d_models, model_types, lambs, alphas, modes):\n",
    "    lamb1, lamb2 = lamb\n",
    "    np.random.seed(123)\n",
    "    tf.random.set_seed(123)\n",
    "    random.seed(123)\n",
    "    \n",
    "    selected_columns = [dataset+\"-O3-AT_IN\", dataset+\"-PM10-AT_IN\", dataset+\"-TMP Media-AT_IN\"]\n",
    "    df = df[selected_columns]\n",
    "    \n",
    "    #for test_year in df.index.year.unique():\n",
    "    return_input_labels = 'multitaskhlnet' in model_type\n",
    "    \n",
    "    if return_input_labels:\n",
    "        df['day_of_week'] = df.index.dayofweek\n",
    "        df['month'] = df.index.month\n",
    "        df['season'] = df.index.quarter\n",
    "        selected_columns.extend(['season', 'month', 'day_of_week'])\n",
    "    \n",
    "    \"\"\"\n",
    "        Data preparation\n",
    "    \"\"\" \n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaler_o3 = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "    train = df[df.index.year!=test_year]\n",
    "\n",
    "    validation_row = int(len(train) * 0.9)\n",
    "    valid = train.iloc[validation_row:, :]\n",
    "    train = train.iloc[:validation_row, :]\n",
    "\n",
    "    test =  df[df.index.year==test_year]\n",
    "\n",
    "    test_dates = test.index\n",
    "\n",
    "    scaler = scaler.fit(np.concatenate((train.values[:, :n_features], valid.values[:, :n_features])))\n",
    "    data_train =  scaler.transform(train.values[:, :n_features])\n",
    "    data_train = np.concatenate((data_train, train.values[:, n_features:]), axis=1)\n",
    "\n",
    "    data_valid = scaler.transform(valid.values[:, :n_features])\n",
    "    data_valid = np.concatenate((data_valid, valid.values[:, n_features:]), axis=1)\n",
    "\n",
    "    data_test = scaler.transform(test.values[:, :n_features])\n",
    "    data_test = np.concatenate((data_test, test.values[:, n_features:]), axis=1)\n",
    "    scaled_o3 = scaler_o3.fit_transform(np.concatenate((train[[target]].values, valid[[target]].values)))\n",
    "\n",
    "    df_train = pd.DataFrame(data = data_train, columns = selected_columns)\n",
    "    df_valid = pd.DataFrame(data = data_valid, columns = selected_columns)\n",
    "    df_test = pd.DataFrame(data = data_test, columns = selected_columns)\n",
    "\n",
    "    window_splitter = partial(split_window, window, horizon, [0], return_input_labels, n_features)\n",
    "\n",
    "    training_generator = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "          data=df_train.values,\n",
    "          targets=None,\n",
    "          sequence_length=window+horizon,\n",
    "          sequence_stride=1,\n",
    "          shuffle=True,\n",
    "          batch_size=batch_size,\n",
    "          seed = 123)\n",
    "    training_generator = training_generator.map(window_splitter, num_parallel_calls=tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    valid_generator = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "          data=df_valid.values,\n",
    "          targets=None,\n",
    "          sequence_length=window+horizon,\n",
    "          sequence_stride=1,\n",
    "          shuffle=True,\n",
    "          batch_size=batch_size,\n",
    "          seed = 123)\n",
    "    valid_generator = valid_generator.map(window_splitter, num_parallel_calls=tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    test_generator = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "          data=df_test.values,\n",
    "          targets=None,\n",
    "          sequence_length=window+horizon,\n",
    "          sequence_stride=1,\n",
    "          shuffle=False,\n",
    "          batch_size=batch_size)\n",
    "\n",
    "    test_generator = test_generator.map(window_splitter, num_parallel_calls=tf.data.AUTOTUNE).cache().prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    model, loss = get_model(model_type, window, horizon, d_model, n_features, batch_size, lamb1, lamb2, grad_smooth_alpha, mode, overall)\n",
    "\n",
    "    radam = tfa.optimizers.RectifiedAdam()\n",
    "    ranger = tfa.optimizers.Lookahead(radam, sync_period=6, slow_step_size=0.5)\n",
    "    model.compile(optimizer=ranger, loss=loss,run_eagerly=True, metrics='mae')\n",
    "\n",
    "    \"\"\"\n",
    "        Callbacks\n",
    "    \"\"\"\n",
    "    base_path = f'../results/{dataset.lower()}/{model_type}/w{window}_h{horizon}_dmodel{d_model}_lamb1{lamb1}_lamb2{lamb2}_a{grad_smooth_alpha}_m{mode}'\n",
    "    print(base_path)\n",
    "    if not os.path.isdir(base_path):\n",
    "        os.makedirs(base_path+'/checkpoints_'+str(test_year))\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True) # Restore brat weights\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(base_path+'/checkpoints_'+str(test_year)+'/cp-{epoch:04d}.ckpt', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='auto')\n",
    "\n",
    "    \"\"\"\n",
    "        Training\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    main_grad_first_level_average, main_grad_second_level_average, first_level_aux_grad_average = None, None, None\n",
    "    \n",
    "    \n",
    "    history = model.fit(training_generator,\n",
    "                        validation_data=valid_generator,\n",
    "                        use_multiprocessing=True,\n",
    "                        callbacks=[checkpoint, early_stopping],\n",
    "                        epochs=epochs,\n",
    "                        workers=12)\n",
    "    \n",
    "    train_time = (time.time() - start_time)/60\n",
    "\n",
    "    history_dict = history.history\n",
    "    json.dump(history_dict, open(base_path+'/checkpoints_'+str(test_year)+'/history.json', 'w'))\n",
    "\n",
    "    \"\"\"\n",
    "        Inference\n",
    "    \"\"\"\n",
    "\n",
    "    #model.load_weights(checkpoint._get_most_recently_modified_file_matching_pattern(base_path+'/checkpoints_'+str(test_year)+'/cp-{epoch:04d}.ckpt'))\n",
    "\n",
    "    reals_raw = df_test[target].values[window:]\n",
    "    indexer = np.arange(horizon)[None, :] + np.arange(len(reals_raw)-horizon+1)[:, None]\n",
    "    reals = reals_raw[indexer]\n",
    "    predictions = model.predict(test_generator)\n",
    "\n",
    "    if pred_index is not None:\n",
    "        predictions = predictions[pred_index]\n",
    "\n",
    "    real_scaled = scaler_o3.inverse_transform(reals)\n",
    "    predictions_scaled = scaler_o3.inverse_transform(predictions)\n",
    "\n",
    "    dates = pd.Series(test_dates.values[window:-horizon+1])\n",
    "    columns = [f'real_{i}' for i in range(horizon)]\n",
    "    columns.extend([f'pred_{i}' for i in range(horizon)])\n",
    "    predictions_df = pd.DataFrame(data=np.concatenate([real_scaled, predictions_scaled], axis=1), columns= columns, index = dates)\n",
    "    metrics_dict = evaluate(real_scaled, predictions_scaled)\n",
    "    metrics = pd.DataFrame(metrics_dict, index = [test_year])\n",
    "    metrics['time'] = train_time\n",
    "\n",
    "    if os.path.exists(base_path+'/metrics.csv') and os.path.exists(base_path+'/predictions.csv'):\n",
    "        metrics.to_csv(base_path+'/metrics.csv', mode='a', header=False)\n",
    "        predictions_df.to_csv(base_path+'/predictions.csv', mode='a', header=False)\n",
    "    else:\n",
    "        metrics.to_csv(base_path+'/metrics.csv')\n",
    "        predictions_df.to_csv(base_path+'/predictions.csv')\n",
    "\n",
    "\n",
    "    print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hacer que los pesos de las tareas se aprendan\n",
    "* Aadir attention\n",
    "* Hacer que los gradientes de un mismo nivel sean coherentes entre ellos\n",
    "* Probar aislar niveles\n",
    "* Aislar solo primer nivel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "PollutionAttention_Bermejales.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
